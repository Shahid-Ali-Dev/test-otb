import os
import requests
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Any
import logging
import re
from collections import Counter
from sqlalchemy import and_
import time
import random

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class YouTubeAnalyzer:
    def __init__(self, api_key: str, db, groq_api_key: str = None):
        self.api_key = api_key
        self.groq_api_key = groq_api_key or os.getenv('GROQ_API_KEY')
        self.youtube_keys = self._initialize_youtube_keys()
        self.groq_keys = self._initialize_groq_keys()
        self.base_url = "https://www.googleapis.com/youtube/v3"
        self.use_ai_calculations = True
        self.db = db
        self.session = requests.Session()
        
    def _initialize_youtube_keys(self):
        """Initialize YouTube API key pool with fallbacks"""
        keys = []
        # Add primary key first
        if self.api_key:
            keys.append(self.api_key)
        
        # Add fallback keys (1-5)
        for i in range(1, 6):
            key = os.getenv(f'YOUTUBE_API_KEY_{i}')
            if key and key not in keys:
                keys.append(key)
        
        # Add any additional keys from environment
        for i in range(6, 10):  # Extend if you have more
            key = os.getenv(f'YOUTUBE_API_KEY_{i}')
            if key and key not in keys:
                keys.append(key)
                
        logger.info(f"üîë Loaded {len(keys)} YouTube API keys")
        return keys
    
    def _initialize_groq_keys(self):
        """Initialize Groq API key pool with fallbacks"""
        keys = []
        # Add primary key first
        if self.groq_api_key:
            keys.append(self.groq_api_key)
        
        # Add fallback keys (1-5)
        for i in range(1, 6):
            key = os.getenv(f'GROQ_API_KEY_{i}')
            if key and key not in keys:
                keys.append(key)
        
        logger.info(f"ü§ñ Loaded {len(keys)} Groq API keys")
        return keys
        
    def _rotate_youtube_key(self, current_key_index: int) -> tuple:
        """FIXED: Proper API key rotation with better error handling"""
        if current_key_index == -1:
            next_index = 0
        else:
            next_index = (current_key_index + 1) % len(self.youtube_keys)
        
        # If we've tried all keys and are back to the first one, wait and reset
        if next_index == 0 and current_key_index != -1:
            logger.warning("üîÑ All YouTube API keys exhausted, waiting 60 seconds")
            time.sleep(60)  # Wait before retrying
            # Reset quota by waiting or implement better quota management
        
        logger.info(f"üîÑ Rotating YouTube API key: {current_key_index} ‚Üí {next_index}")
        return self.youtube_keys[next_index], next_index
    
    def _rotate_groq_key(self, current_key_index: int) -> tuple:
        """Rotate to next Groq API key"""
        next_index = (current_key_index + 1) % len(self.groq_keys)
        if next_index == 0 and current_key_index != -1:
            logger.error("‚ùå All Groq API keys exhausted")
            return None, -1
        logger.info(f"üîÑ Rotating Groq API key: {current_key_index} ‚Üí {next_index}")
        return self.groq_keys[next_index], next_index
        
    def analyze_channel(self, channel_id: str, force_refresh: bool = False) -> Dict:
        """Comprehensive channel analysis with enhanced data extraction and error handling"""
        try:
            logger.info(f"üéØ Starting analysis for channel: {channel_id}, force_refresh: {force_refresh}")
            
            # Check for recent analysis (with database lock handling)
            if not force_refresh:
                recent_analysis = self.get_recent_analysis_safe(channel_id)
                if recent_analysis:
                    logger.info(f"‚úÖ Using cached analysis for channel {channel_id}")
                    return recent_analysis
                else:
                    logger.warning(f"‚ùå No cached data found, proceeding with fresh analysis")
            
            # üî• If we get here, either force_refresh=True OR no cached data found
            logger.info(f"üîÑ Starting comprehensive analysis for channel {channel_id}")
            
            # Get enhanced channel data with retry mechanism
            channel_data = self.get_enhanced_channel_data_with_retry(channel_id)
            logger.info(f"üìä Channel data retrieved: {bool(channel_data)}")
            
            if not channel_data:
                logger.warning(f"‚ùå No channel data found for {channel_id}, using enhanced fallback")
                return self.get_enhanced_fallback_analysis(channel_id)
            
            # Get extensive video data with safe limits
            videos_data = self.get_channel_videos_safe(channel_id, max_results=30)
            logger.info(f"üé¨ Videos data retrieved: {bool(videos_data)}")

            videos_data = self.get_channel_videos_safe(channel_id, max_results=30)
            logger.info(f"üé¨ Videos data retrieved: {bool(videos_data)}")
            
            
            # Calculate comprehensive metrics
            engagement_metrics = self.calculate_comprehensive_engagement(videos_data)
            content_analysis = self.analyze_content_strategy_enhanced(videos_data, channel_data)
            performance_metrics = self.get_performance_metrics_safe(videos_data, engagement_metrics)
            
            # üî• DEBUG: Check what we have so far
            logger.info("=== METRICS CALCULATION DEBUG ===")
            logger.info(f"Engagement metrics keys: {list(engagement_metrics.keys())}")
            logger.info(f"Performance metrics: {performance_metrics}")
            logger.info(f"Content analysis keys: {list(content_analysis.keys())}")


            
            # Generate AI-enhanced insights with fallback
            ai_metrics = self.calculate_ai_enhanced_metrics(channel_data, engagement_metrics, content_analysis, performance_metrics)
            demographics = self.infer_demographics_enhanced(channel_data, videos_data, content_analysis)
            insights = self.generate_ai_insights_enhanced(channel_data, ai_metrics, content_analysis, performance_metrics)
            predictions = self.predict_growth_enhanced(channel_data, ai_metrics, engagement_metrics)
            recommendations = self.generate_recommendations_enhanced(channel_data, insights, content_analysis, ai_metrics)
            
            # Compile comprehensive analysis
            analysis = {
                'channel_metrics': channel_data,
                'engagement_metrics': engagement_metrics,
                'performance_metrics': performance_metrics,
                'content_analysis': content_analysis,
                'growth_metrics': self.calculate_growth_metrics(channel_data, engagement_metrics),
                'ai_enhanced_metrics': ai_metrics,
                'demographics': demographics,
                'ai_insights': insights,
                'growth_predictions': predictions,
                'recommendations': recommendations,
                'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                'data_source': 'youtube_api_v3'
            }
            
            logger.info(f"üì¶ Analysis compiled, saving to database...")

            # üî• DEBUG: Final check before saving
            logger.info("=== FINAL ANALYSIS DEBUG ===")
            logger.info(f"Channel metrics: {analysis['channel_metrics'].get('subscribers', 0)} subscribers")
            logger.info(f"Engagement rate: {analysis['engagement_metrics'].get('avg_engagement_rate', 0)}%")
            logger.info(f"Performance trend: {analysis['performance_metrics'].get('performance_trend', 'Unknown')}")
            logger.info(f"Optimal length: {analysis['performance_metrics'].get('optimal_video_length', 'Unknown')}")
            logger.info(f"Top video views: {analysis['engagement_metrics'].get('top_performing_video_views', 0)}")
            logger.info(f"Engagement consistency: {analysis['engagement_metrics'].get('engagement_consistency', 0)}%")

            # Save to database with safe handling
            self.save_analysis_safe(channel_id, analysis)
            
            logger.info(f"‚úÖ Analysis completed successfully for channel {channel_id}")
            # In analyze_channel method, before return analysis:
            analysis = self.unify_data_sources(analysis)
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå Error analyzing channel {channel_id}: {str(e)}")
            import traceback
            traceback.print_exc()
            return self.get_enhanced_fallback_analysis(channel_id)
        
    def debug_performance_trend(self, performances: List[float]):
        """Debug method to see why Rapid Decline is triggering"""
        if not performances:
            logger.info("üîç DEBUG: No performances data")
            return
        
        recent = performances[:10]
        logger.info(f"üîç DEBUG: Recent performances: {recent}")
        
        if len(recent) >= 5:
            half = len(recent) // 2
            first = recent[half:]
            second = recent[:half]
            
            first_avg = np.mean(first)
            second_avg = np.mean(second)
            change = ((second_avg - first_avg) / first_avg) * 100 if first_avg > 0 else 0
            
            logger.info(f"üîç DEBUG: First half avg: {first_avg:.0f}, Second half avg: {second_avg:.0f}")
            logger.info(f"üîç DEBUG: Percentage change: {change:.1f}%")
            logger.info(f"üîç DEBUG: Would return: {self.analyze_performance_trend_enhanced(performances)}")
            
    def validate_insights_consistency(self, insights: List[Dict], analysis: Dict) -> List[Dict]:
        """Remove contradictory insights"""
        valid_insights = []
        
        engagement_rate = analysis['engagement_metrics'].get('avg_engagement_rate', 0)
        consistency = analysis['engagement_metrics'].get('performance_consistency', 0)
        viral_ratio = analysis['ai_enhanced_metrics'].get('viral_ratio', 0)
        
        for insight in insights:
            description = insight.get('description', '').lower()
            title = insight.get('title', '').lower()
            
            # Remove insights that contradict viral success
            if viral_ratio > 5 and any(word in description for word in ['decline', 'failing', 'poor', 'struggling']):
                continue
                
            # Remove insights that contradict good engagement
            if engagement_rate >= 4 and any(word in description for word in ['low engagement', 'poor engagement']):
                continue
                
            # Remove insights that contradict consistency data
            if consistency < 20 and any(word in description for word in ['consistent', 'stable', 'reliable']):
                continue
                
            valid_insights.append(insight)
        
        return valid_insights[:3]  # Ensure we return exactly 3
    
    def get_recent_analysis_safe(self, channel_id: str) -> Optional[Dict]:
        """Get recent analysis from database with safe handling"""
        try:
            from models import YouTubeAnalyticsSnapshot
            from datetime import datetime, timedelta
            
            # Check for analysis in the last 7 days
            recent_cutoff = datetime.now() - timedelta(days=7)
            
            logger.info(f"üîç Looking for cached data for channel {channel_id} since {recent_cutoff}")
            
            recent_snapshot = YouTubeAnalyticsSnapshot.query.filter(
                and_(
                    YouTubeAnalyticsSnapshot.channel_id == channel_id,
                    YouTubeAnalyticsSnapshot.analysis_timestamp >= recent_cutoff
                )
            ).order_by(YouTubeAnalyticsSnapshot.analysis_timestamp.desc()).first()
            
            if recent_snapshot:
                logger.info(f"‚úÖ Found cached analysis from {recent_snapshot.analysis_timestamp}")
                
                # üî• DEBUG: Check what's in the snapshot
                logger.info("=== CACHED SNAPSHOT DEBUG ===")
                logger.info(f"Snapshot subscribers: {recent_snapshot.subscribers}")
                logger.info(f"Snapshot optimal_length: {recent_snapshot.optimal_video_length}")
                logger.info(f"Snapshot performance_trend: {getattr(recent_snapshot, 'performance_trend', 'MISSING')}")
                
                return self.convert_snapshot_to_analysis(recent_snapshot)
            else:
                logger.warning(f"‚ùå No cached analysis found for channel {channel_id}")
                
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Error getting recent analysis: {str(e)}")
            return None
    
    def get_performance_metrics_safe(self, videos_data: Dict, engagement_metrics: Dict) -> Dict:
        """Get performance metrics with comprehensive fallbacks"""
        try:
            return self.calculate_performance_metrics_enhanced(videos_data, engagement_metrics)
        except Exception as e:
            logger.error(f"‚ùå Error calculating performance metrics: {str(e)}")
            return self.get_default_performance_metrics()

    def convert_snapshot_to_analysis(self, snapshot) -> Dict:
        """Convert database snapshot to analysis format safely - ROBUST VERSION"""
        try:
            # üî• SAFE ACCESS FOR ALL POTENTIALLY MISSING COLUMNS
            def safe_get_attr(obj, attr_name, default=None):
                try:
                    value = getattr(obj, attr_name, default)
                    return value if value is not None else default
                except:
                    return default
            
            top_video_views = safe_get_attr(snapshot, 'top_performing_video_views', 0)
            
            logger.info("=== SNAPSHOT CONVERSION DEBUG ===")
            logger.info(f"Converting snapshot: {snapshot.id}")
            logger.info(f"Snapshot optimal_length: {safe_get_attr(snapshot, 'optimal_video_length')}")
            logger.info(f"Snapshot performance_trend: {safe_get_attr(snapshot, 'performance_trend')}")
            logger.info(f"Snapshot engagement_consistency: {safe_get_attr(snapshot, 'engagement_consistency')}")
            logger.info(f"Snapshot top_video_views: {safe_get_attr(snapshot, 'top_performing_video_views')}")
            
            analysis = {
                'channel_metrics': {
                    'channel_title': safe_get_attr(snapshot, 'channel_title', 'YouTube Channel'),
                    'channel_description': safe_get_attr(snapshot, 'channel_description', ''),
                    'subscribers': safe_get_attr(snapshot, 'subscribers', 0),
                    'total_views': safe_get_attr(snapshot, 'total_views', 0),
                    'total_videos': safe_get_attr(snapshot, 'total_videos', 0),
                    'channel_age_days': safe_get_attr(snapshot, 'channel_age_days', 0),
                    'country': safe_get_attr(snapshot, 'channel_country', 'Unknown'),
                    'custom_url': safe_get_attr(snapshot, 'channel_custom_url', ''),
                    'published_at': safe_get_attr(snapshot, 'channel_published_at'),
                },
                'engagement_metrics': {
                    'avg_engagement_rate': safe_get_attr(snapshot, 'avg_engagement_rate', 0),
                    'engagement_health': safe_get_attr(snapshot, 'engagement_health', 'Unknown'),
                    'total_recent_likes': safe_get_attr(snapshot, 'total_likes', 0),
                    'total_recent_comments': safe_get_attr(snapshot, 'total_comments', 0),
                    'videos_analyzed': safe_get_attr(snapshot, 'videos_analyzed', 0),
                    'avg_views_per_video': safe_get_attr(snapshot, 'avg_views_per_video', 0),
                    'avg_likes_per_video': safe_get_attr(snapshot, 'avg_likes_per_video', 0),
                    'avg_comments_per_video': safe_get_attr(snapshot, 'avg_comments_per_video', 0),
                    'performance_consistency': safe_get_attr(snapshot, 'performance_consistency', 0),
                    'total_recent_views': safe_get_attr(snapshot, 'total_views', 0),
                    'total_engagement': safe_get_attr(snapshot, 'total_engagement', 0),
                    'views_std_dev': safe_get_attr(snapshot, 'views_std_dev', 0),
                    'top_performing_video_views': top_video_views,
                    'top_performing_video_likes': safe_get_attr(snapshot, 'top_performing_video_likes', 0),
                    'top_performing_video_title': safe_get_attr(snapshot, 'top_performing_video_title', ''),
                    'engagement_consistency': safe_get_attr(snapshot, 'engagement_consistency', 0),
                },
                'performance_metrics': {
                    'avg_duration_seconds': safe_get_attr(snapshot, 'avg_duration_seconds', 0),
                    'optimal_video_length': safe_get_attr(snapshot, 'optimal_video_length', 'Unknown'),
                    'performance_trend': safe_get_attr(snapshot, 'performance_trend', 'Unknown'),
                    'avg_performance_score': safe_get_attr(snapshot, 'avg_performance_score', 0),
                    'duration_consistency': safe_get_attr(snapshot, 'duration_consistency', 0),
                    'estimated_retention_rate': safe_get_attr(snapshot, 'estimated_retention_rate', 0),
                    'content_velocity_score': safe_get_attr(snapshot, 'content_velocity_score', 0),
                },
                'ai_enhanced_metrics': {
                    'channel_health_score': safe_get_attr(snapshot, 'channel_health_score', 0),
                    'performance_tier': safe_get_attr(snapshot, 'performance_tier', 'Unknown'),
                    'growth_potential': safe_get_attr(snapshot, 'growth_potential', 'Unknown'),
                    'content_quality_score': safe_get_attr(snapshot, 'content_quality_score', 0),
                },
                'content_analysis': {
                    'content_categories': safe_get_attr(snapshot, 'content_categories', {}),
                    'publishing_frequency': safe_get_attr(snapshot, 'publishing_frequency', 'Unknown'),
                    'optimal_video_length': safe_get_attr(snapshot, 'optimal_video_length', 'Unknown'),
                    'content_gaps': safe_get_attr(snapshot, 'content_gaps', []),
                    'trending_alignment_score': safe_get_attr(snapshot, 'trending_topics_alignment', 0),
                    'content_diversity_score': safe_get_attr(snapshot, 'content_diversity_score', 0),
                    'total_videos_analyzed': safe_get_attr(snapshot, 'videos_analyzed', 0),
                },
                'demographics': {
                    'age_groups': safe_get_attr(snapshot, 'estimated_age_groups', {}),
                    'gender_ratio': safe_get_attr(snapshot, 'estimated_gender_ratio', {}),
                    'geographic_distribution': safe_get_attr(snapshot, 'geographic_distribution', {}),
                    'interests': safe_get_attr(snapshot, 'audience_interests', []),
                },
                'ai_insights': safe_get_attr(snapshot, 'ai_insights', []),
                'growth_predictions': safe_get_attr(snapshot, 'growth_predictions', {}),
                'recommendations': safe_get_attr(snapshot, 'recommendations', []),
                'data_source': 'cached_7day'
            }
            
            logger.info("‚úÖ Snapshot conversion completed successfully")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå Error converting snapshot: {str(e)}")
            import traceback
            traceback.print_exc()
            return self.get_enhanced_fallback_analysis("")

    def get_enhanced_fallback_analysis(self, channel_id: str) -> Dict:
        """Provide enhanced fallback analysis when API calls fail"""
        logger.warning(f"üîÑ Using enhanced fallback analysis for channel {channel_id}")
        
        return {
            'channel_metrics': {
                'channel_title': 'YouTube Channel',
                'subscribers': 0,
                'total_views': 0,
                'total_videos': 0,
                'channel_age_days': 0,
                'country': 'Unknown',
                'custom_url': '',
            },
            'engagement_metrics': {
                'avg_engagement_rate': 0,
                'engagement_health': 'Unknown',
                'total_recent_likes': 0,
                'total_recent_comments': 0,
                'videos_analyzed': 0,
                'avg_views_per_video': 0,
                'avg_likes_per_video': 0,
                'avg_comments_per_video': 0,
                'performance_consistency': 0,
                'views_std_dev': 0,
                'top_performing_video_views': 0,
                'top_performing_video_likes': 0,
                'top_performing_video_title': '',
                'engagement_consistency': 0,
            },
            'performance_metrics': {
                'avg_duration_seconds': 0,
                'optimal_video_length': 'Unknown',
                'performance_trend': 'Unknown',
                'avg_performance_score': 0,
                'duration_consistency': 0,
                'estimated_retention_rate': 0,
                'content_velocity_score': 0,
            },
            'ai_enhanced_metrics': {
                'channel_health_score': 0,
                'performance_tier': 'Unknown',
                'growth_potential': 'Unknown',
                'content_quality_score': 0,
            },
            'content_analysis': {
                'content_categories': {},
                'publishing_frequency': 'Unknown',
                'optimal_video_length': 'Unknown',
                'content_gaps': [],
                'trending_alignment_score': 0,
                'content_diversity_score': 0,
                'total_videos_analyzed': 0,
            },
            'demographics': {
                'age_groups': {'18-24': 35, '25-34': 40, '35-44': 15, '45+': 10},
                'gender_ratio': {'male': 65, 'female': 35},
                'geographic_distribution': {'US': 40, 'UK': 15, 'India': 10, 'Other': 35},
                'interests': ['Technology', 'Education', 'Tutorials'],
            },
            'ai_insights': self.generate_basic_insights_fallback(),
            'growth_predictions': {},
            'recommendations': [],
            'data_source': 'fallback'
        }

    def flatten_youtube_data(self, analysis: Dict) -> Dict:
        """Flatten the nested YouTube analysis structure for frontend template"""
        flattened = {}
        
        # Channel metrics
        channel_metrics = analysis.get('channel_metrics', {})
        flattened.update({
            'channel_title': channel_metrics.get('channel_title', 'YouTube Channel'),
            'subscribers': channel_metrics.get('subscribers', 0),
            'total_views': channel_metrics.get('total_views', 0),
            'total_videos': channel_metrics.get('total_videos', 0),
            'channel_age_days': channel_metrics.get('channel_age_days', 0),
            'custom_url': channel_metrics.get('custom_url', ''),
            'country': channel_metrics.get('country', 'Unknown'),
        })
        
        # Engagement metrics
        engagement_metrics = analysis.get('engagement_metrics', {})
        flattened.update({
            'engagement_rate': engagement_metrics.get('avg_engagement_rate', 0),
            'engagement_health': engagement_metrics.get('engagement_health', 'Unknown'),
            'engagement_consistency': engagement_metrics.get('engagement_consistency', 0),
            'total_likes': engagement_metrics.get('total_recent_likes', 0),
            'total_comments': engagement_metrics.get('total_recent_comments', 0),
            'total_engagement': engagement_metrics.get('total_engagement', 0),
            'videos_analyzed': engagement_metrics.get('videos_analyzed', 0),
            'performance_consistency': engagement_metrics.get('performance_consistency', 0),
            'views_std_dev': engagement_metrics.get('views_std_dev', 0),
            'avg_views_per_video': engagement_metrics.get('avg_views_per_video', 0),
            'avg_likes_per_video': engagement_metrics.get('avg_likes_per_video', 0),
            'avg_comments_per_video': engagement_metrics.get('avg_comments_per_video', 0),
            'top_performing_video_views': engagement_metrics.get('top_performing_video_views', 0),
            'top_performing_video_likes': engagement_metrics.get('top_performing_video_likes', 0),
            'top_performing_video_title': engagement_metrics.get('top_performing_video_title', ''),
        })
        
        # Performance metrics
        performance_metrics = analysis.get('performance_metrics', {})
        flattened.update({
            'avg_duration_seconds': performance_metrics.get('avg_duration_seconds', 0),
            'optimal_video_length': performance_metrics.get('optimal_video_length', 'Unknown'),
            'performance_trend': performance_metrics.get('performance_trend', 'Unknown'),
            'avg_performance_score': performance_metrics.get('avg_performance_score', 0),
            'duration_consistency': performance_metrics.get('duration_consistency', 0),
            'estimated_retention_rate': performance_metrics.get('estimated_retention_rate', 0),
            'content_velocity_score': performance_metrics.get('content_velocity_score', 0),
        })
        
        # AI-enhanced metrics
        ai_metrics = analysis.get('ai_enhanced_metrics', {})
        flattened.update({
            'channel_health': ai_metrics.get('channel_health_score', 0),
            'performance_tier': ai_metrics.get('performance_tier', 'Unknown'),
            'growth_potential': ai_metrics.get('growth_potential', 'Unknown'),
            'content_quality_score': ai_metrics.get('content_quality_score', 0),
            'audience_loyalty_score': ai_metrics.get('audience_loyalty_score', 0),
            'algorithm_favorability': ai_metrics.get('algorithm_favorability', 0),
        })
        
        # Content analysis
        content_analysis = analysis.get('content_analysis', {})
        flattened.update({
            'content_analysis': content_analysis,
            'content_diversity_score': content_analysis.get('content_diversity_score', 0),
        })
        
        # Other data
        flattened.update({
            'demographics': analysis.get('demographics', {}),
            'ai_insights': analysis.get('ai_insights', []),
            'growth_predictions': analysis.get('growth_predictions', {}),
            'recommendations': analysis.get('recommendations', []),
            'data_source': analysis.get('data_source', 'unknown'),
        })
        
        return flattened

    def save_analysis_safe(self, channel_id: str, analysis: Dict):
        """Save analysis to database with safe handling"""
        try:
            from models import YouTubeAnalyticsSnapshot
            
            # Use a separate session to avoid locking issues
            from sqlalchemy.orm import sessionmaker
            
            Session = sessionmaker(bind=self.db.engine)
            session = Session()
            
            try:
                # Check for existing entry
                existing = session.query(YouTubeAnalyticsSnapshot).filter(
                    and_(
                        YouTubeAnalyticsSnapshot.channel_id == channel_id,
                        YouTubeAnalyticsSnapshot.snapshot_date == datetime.now(timezone.utc).date()
                    )
                ).first()
                
                if existing:
                    snapshot = existing
                    logger.info(f"üìù Updating existing snapshot for channel {channel_id}")
                else:
                    snapshot = YouTubeAnalyticsSnapshot(
                        channel_id=channel_id,
                        snapshot_date=datetime.now(timezone.utc).date()
                    )
                    session.add(snapshot)
                    logger.info(f"üÜï Creating new snapshot for channel {channel_id}")
                
                # Populate snapshot data
                self.populate_snapshot_data(snapshot, analysis)
                
                # Commit with timeout
                session.commit()
                logger.info(f"üíæ Analysis saved successfully for channel {channel_id}")
                
            except Exception as e:
                session.rollback()
                logger.error(f"‚ùå Error in database operation: {str(e)}")
                
            finally:
                session.close()
                
        except Exception as e:
            logger.error(f"‚ùå Error in save_analysis_safe: {str(e)}")

    def populate_snapshot_data(self, snapshot, analysis: Dict):
        """Populate snapshot data from analysis - COMPLETE VERSION"""
        try:
            logger.info("=== POPULATING SNAPSHOT DATA ===")
            
            # Channel metrics
            channel_metrics = analysis['channel_metrics']
            snapshot.channel_title = channel_metrics.get('channel_title', '')
            snapshot.channel_description = channel_metrics.get('channel_description', '')
            snapshot.channel_custom_url = channel_metrics.get('custom_url', '')
            snapshot.channel_country = channel_metrics.get('country', 'Unknown')
            snapshot.subscribers = channel_metrics.get('subscribers', 0)
            snapshot.total_views = channel_metrics.get('total_views', 0)
            snapshot.total_videos = channel_metrics.get('total_videos', 0)
            snapshot.channel_age_days = channel_metrics.get('channel_age_days', 0)
            
            # Handle channel_published_at
            published_at = channel_metrics.get('published_at')
            if published_at:
                try:
                    snapshot.channel_published_at = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                except:
                    snapshot.channel_published_at = None
            
            # Engagement metrics - COMPLETE MAPPING
            engagement_metrics = analysis['engagement_metrics']
            snapshot.total_likes = engagement_metrics.get('total_recent_likes', 0)
            snapshot.total_comments = engagement_metrics.get('total_recent_comments', 0)
            snapshot.total_engagement = engagement_metrics.get('total_engagement', 0)
            snapshot.avg_engagement_rate = engagement_metrics.get('avg_engagement_rate', 0)
            snapshot.engagement_health = engagement_metrics.get('engagement_health', 'Unknown')
            
            # üî• ADD THESE MISSING ENGAGEMENT METRICS
            snapshot.engagement_consistency = engagement_metrics.get('engagement_consistency', 0)
            snapshot.avg_views_per_video = engagement_metrics.get('avg_views_per_video', 0)
            snapshot.avg_likes_per_video = engagement_metrics.get('avg_likes_per_video', 0)
            snapshot.avg_comments_per_video = engagement_metrics.get('avg_comments_per_video', 0)
            snapshot.performance_consistency = engagement_metrics.get('performance_consistency', 0)
            snapshot.views_std_dev = engagement_metrics.get('views_std_dev', 0)
            snapshot.top_performing_video_views = engagement_metrics.get('top_performing_video_views', 0)
            snapshot.top_performing_video_likes = engagement_metrics.get('top_performing_video_likes', 0)
            snapshot.top_performing_video_title = engagement_metrics.get('top_performing_video_title', '')
            
            # Performance metrics
            performance_metrics = analysis.get('performance_metrics', {})
            snapshot.avg_duration_seconds = performance_metrics.get('avg_duration_seconds', 0)
            snapshot.optimal_video_length = performance_metrics.get('optimal_video_length', 'Unknown')
            snapshot.performance_trend = performance_metrics.get('performance_trend', 'Unknown')
            snapshot.avg_performance_score = performance_metrics.get('avg_performance_score', 0)
            snapshot.duration_consistency = performance_metrics.get('duration_consistency', 0)
            snapshot.estimated_retention_rate = performance_metrics.get('estimated_retention_rate', 0)
            snapshot.content_velocity_score = performance_metrics.get('content_velocity_score', 0)
            
            # AI metrics
            ai_metrics = analysis['ai_enhanced_metrics']
            snapshot.channel_health_score = ai_metrics.get('channel_health_score', 0)
            snapshot.performance_tier = ai_metrics.get('performance_tier', 'Unknown')
            snapshot.content_quality_score = ai_metrics.get('content_quality_score', 0)
            snapshot.growth_potential = ai_metrics.get('growth_potential', 'Unknown')
            
            # Content analysis
            content_analysis = analysis['content_analysis']
            snapshot.content_categories = content_analysis.get('content_categories', {})
            snapshot.publishing_frequency = content_analysis.get('publishing_frequency', 'Unknown')
            snapshot.content_gaps = content_analysis.get('content_gaps', [])
            snapshot.trending_topics_alignment = content_analysis.get('trending_alignment_score', 0)
            
            # Enhanced content metrics
            snapshot.content_diversity_score = content_analysis.get('content_diversity_score', 0)
            snapshot.content_freshness_score = content_analysis.get('content_freshness_score', 0)
            snapshot.publishing_consistency = content_analysis.get('publishing_consistency', 0)
            
            # Demographics
            demographics = analysis['demographics']
            snapshot.estimated_age_groups = demographics.get('age_groups', {})
            snapshot.estimated_gender_ratio = demographics.get('gender_ratio', {})
            snapshot.geographic_distribution = demographics.get('geographic_distribution', {})
            snapshot.audience_interests = demographics.get('interests', [])
            
            # AI insights
            snapshot.ai_insights = analysis.get('ai_insights', [])
            snapshot.growth_predictions = analysis.get('growth_predictions', {})
            snapshot.recommendations = analysis.get('recommendations', [])
            
            snapshot.videos_analyzed = engagement_metrics.get('videos_analyzed', 0)
            snapshot.data_source = analysis.get('data_source', 'unknown')
            snapshot.analysis_timestamp = datetime.now(timezone.utc)
            
            # üî• DEBUG: Log what we're saving
            logger.info("=== SAVING TO DATABASE ===")
            logger.info(f"Engagement consistency: {snapshot.engagement_consistency}")
            logger.info(f"Top video views: {snapshot.top_performing_video_views}")
            logger.info(f"Top video title: {snapshot.top_performing_video_title}")
            logger.info(f"Performance trend: {snapshot.performance_trend}")
            logger.info(f"Optimal length: {snapshot.optimal_video_length}")
            
            logger.info("‚úÖ Snapshot data populated successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Error populating snapshot data: {str(e)}")
            import traceback
            traceback.print_exc()

    # ==================== API METHODS ====================
    
    def get_enhanced_channel_data_with_retry(self, channel_id: str, max_retries: int = 3) -> Optional[Dict]:
        """FIXED: Get channel data with PROPER API key rotation"""
        # First, resolve custom URLs with rotation
        if channel_id.startswith('@'):
            logger.info(f"üîÑ Resolving custom URL first: {channel_id}")
            resolved_id = self.resolve_custom_url_to_channel_id(channel_id)
            if resolved_id:
                channel_id = resolved_id
                logger.info(f"‚úÖ Resolved to channel ID: {channel_id}")
            else:
                logger.error(f"‚ùå Failed to resolve custom URL: {channel_id}")
                return None
        
        # Now fetch channel data with API rotation
        for key_index, current_key in enumerate(self.youtube_keys):
            for attempt in range(max_retries):
                try:
                    logger.info(f"üîë Using YouTube API key {key_index + 1}/{len(self.youtube_keys)} (attempt {attempt + 1})")
                    
                    url = f"{self.base_url}/channels"
                    params = {
                        'part': 'snippet,statistics,contentDetails,brandingSettings',
                        'key': current_key
                    }
                    
                    # Use channel ID directly (already resolved if it was a custom URL)
                    if channel_id.startswith('UC') and len(channel_id) == 24:
                        params['id'] = channel_id
                    else:
                        # Fallback: try as custom URL
                        params['forHandle'] = channel_id if channel_id.startswith('@') else f"@{channel_id}"
                    
                    response = self.session.get(url, params=params, timeout=30)
                    data = response.json()
                    
                    # Check for API key errors
                    if 'error' in data:
                        error_message = data['error'].get('message', 'Unknown error')
                        error_code = data['error'].get('code')
                        
                        # Check if it's an API key error
                        if error_code in [403, 400, 401, 429] and any(keyword in error_message.lower() for keyword in 
                                                                ['quota', 'exceeded', 'disabled', 'forbidden', 'invalid', 'key']):
                            logger.warning(f"‚ùå YouTube API key {key_index + 1} failed: {error_message}")
                            break  # Break retry loop, try next key
                        else:
                            logger.error(f"‚ùå YouTube API error: {error_message}")
                            if attempt < max_retries - 1:
                                time.sleep(2 ** attempt)
                                continue
                            else:
                                break
                    
                    if not data.get('items'):
                        logger.warning(f"‚ùå No channel found for: {channel_id} with key {key_index + 1}")
                        if attempt < max_retries - 1:
                            time.sleep(2 ** attempt)
                            continue
                        else:
                            # Try next API key
                            break
                    
                    # Success! Process the channel data
                    channel = data['items'][0]
                    stats = channel.get('statistics', {})
                    snippet = channel.get('snippet', {})
                    branding = channel.get('brandingSettings', {})
                    
                    # Extract thumbnails
                    thumbnails = snippet.get('thumbnails', {})
                    thumbnail_url = None
                    if thumbnails.get('high'):
                        thumbnail_url = thumbnails['high']['url']
                    elif thumbnails.get('medium'):
                        thumbnail_url = thumbnails['medium']['url']
                    elif thumbnails.get('default'):
                        thumbnail_url = thumbnails['default']['url']
                    
                    # Check if channel is verified
                    is_verified = branding.get('channel', {}).get('unsubscribedTrailer', '') != ''
                    
                    logger.info(f"‚úÖ Successfully fetched channel data with key {key_index + 1}")
                    return {
                        'channel_id': channel['id'],
                        'channel_title': snippet.get('title', ''),
                        'channel_description': snippet.get('description', ''),
                        'thumbnail_url': thumbnail_url,
                        'is_verified': is_verified,
                        'published_at': snippet.get('publishedAt', ''),
                        'country': snippet.get('country', 'Unknown'),
                        'custom_url': snippet.get('customUrl', ''),
                        'subscribers': int(stats.get('subscriberCount', 0)),
                        'total_views': int(stats.get('viewCount', 0)),
                        'total_videos': int(stats.get('videoCount', 0)),
                        'hidden_subscribers': stats.get('hiddenSubscriberCount', False),
                        'channel_age_days': self.calculate_channel_age(snippet.get('publishedAt', '')),
                        'keywords': branding.get('channel', {}).get('keywords', ''),
                        'featured_channels': branding.get('channel', {}).get('featuredChannelsUrls', [])
                    }
                        
                except requests.exceptions.RequestException as e:
                    logger.warning(f"‚ùå Network error with key {key_index + 1}, attempt {attempt + 1}: {str(e)}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                        continue
                    else:
                        break
                except Exception as e:
                    logger.warning(f"‚ùå Error with key {key_index + 1}, attempt {attempt + 1}: {str(e)}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                        continue
                    else:
                        break
        
        logger.error("‚ùå All YouTube API keys exhausted")
        return None

    def resolve_custom_url_to_channel_id(self, identifier: str, api_key: str = None) -> Optional[str]:
        """FIXED: Resolve custom URLs with proper API key rotation"""
        if not identifier.startswith('@'):
            return None
        
        try:
            logger.info(f"üîÑ [DEBUG] Resolving custom URL: {identifier}")
            
            # Remove @ symbol for search
            search_query = identifier[1:]
            
            # Try all API keys for search
            for key_index, current_key in enumerate(self.youtube_keys):
                try:
                    logger.info(f"üîë [DEBUG] Trying YouTube API key {key_index + 1}/{len(self.youtube_keys)} for search")
                    
                    url = f"{self.base_url}/search"
                    params = {
                        'part': 'snippet',
                        'q': search_query,
                        'type': 'channel',
                        'maxResults': 10,
                        'key': current_key
                    }
                    
                    response = self.session.get(url, params=params, timeout=30)
                    data = response.json()
                    
                    # Check for API key errors
                    if 'error' in data:
                        error_message = data['error'].get('message', 'Unknown error')
                        error_code = data['error'].get('code')
                        
                        # Check if it's an API key error
                        if error_code in [403, 400, 401, 429] and any(keyword in error_message.lower() for keyword in 
                                                                ['quota', 'exceeded', 'disabled', 'forbidden', 'invalid', 'key']):
                            logger.warning(f"‚ùå [DEBUG] YouTube API key {key_index + 1} failed for search: {error_message}")
                            continue  # Try next key
                        else:
                            logger.error(f"‚ùå [DEBUG] Search API error: {error_message}")
                            break
                    
                    logger.info(f"üîç [DEBUG] Search found {len(data.get('items', []))} channels for: {search_query}")
                    
                    if data.get('items'):
                        # Try to find exact or close matches
                        exact_matches = []
                        close_matches = []
                        
                        for item in data['items']:
                            channel_title = item['snippet']['title'].lower()
                            channel_description = item['snippet'].get('description', '').lower()
                            
                            # Check for exact match in title
                            if search_query.lower() == channel_title:
                                exact_matches.append(item)
                            # Check for close match in title
                            elif search_query.lower() in channel_title:
                                close_matches.append(item)
                            # Check in description
                            elif search_query.lower() in channel_description:
                                close_matches.append(item)
                        
                        # Priority: exact matches > close matches > first result
                        if exact_matches:
                            best_match = exact_matches[0]
                            logger.info(f"‚úÖ [DEBUG] Found EXACT match: {best_match['snippet']['title']}")
                        elif close_matches:
                            best_match = close_matches[0]
                            logger.info(f"‚úÖ [DEBUG] Found CLOSE match: {best_match['snippet']['title']}")
                        elif data['items']:
                            best_match = data['items'][0]
                            logger.info(f"‚úÖ [DEBUG] Using FIRST result: {best_match['snippet']['title']}")
                        else:
                            continue  # No matches, try next API key
                        
                        channel_id = best_match['snippet']['channelId']
                        logger.info(f"‚úÖ [DEBUG] Resolved {identifier} to channel ID: {channel_id} using key {key_index + 1}")
                        return channel_id
                    else:
                        logger.warning(f"‚ùå [DEBUG] No search results with key {key_index + 1} for: {search_query}")
                        continue  # Try next API key
                        
                except requests.exceptions.RequestException as e:
                    logger.warning(f"‚ùå [DEBUG] Network error with key {key_index + 1}: {str(e)}")
                    continue
                except Exception as e:
                    logger.warning(f"‚ùå [DEBUG] Error with key {key_index + 1}: {str(e)}")
                    continue
            
            logger.error(f"‚ùå [DEBUG] All YouTube API keys exhausted for search: {search_query}")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå [DEBUG] Error resolving custom URL: {str(e)}")
            return None

    def get_channel_by_search(self, query: str, api_key: str) -> Optional[Dict]:
        """Enhanced fallback method to find channel by search"""
        try:
            # Clean the query
            clean_query = query.replace('@', '').strip()
            
            url = f"{self.base_url}/search"
            params = {
                'part': 'snippet',
                'q': clean_query,
                'type': 'channel',
                'maxResults': 3,
                'key': api_key
            }
            
            response = self.session.get(url, params=params, timeout=30)
            data = response.json()
            
            if data.get('items'):
                # Try to find the best match
                best_match = None
                for item in data['items']:
                    channel_title = item['snippet']['title'].lower()
                    if clean_query.lower() in channel_title:
                        best_match = item
                        break
                
                # If no close match, use the first result
                if not best_match and data['items']:
                    best_match = data['items'][0]
                
                if best_match:
                    channel_id = best_match['snippet']['channelId']
                    logger.info(f"‚úÖ Found channel via search: {channel_id}")
                    # Recursively call with the found channel ID
                    return self.get_enhanced_channel_data_with_retry(channel_id)
            
            logger.warning(f"‚ùå No channel found via search for: {clean_query}")
            return None
            
        except Exception as e:
            logger.error(f"Search fallback failed: {str(e)}")
            return None
        
    def get_channel_videos_safe(self, channel_id: str, max_results: int = 50) -> Dict:
        """FIXED: Get channel videos with PROPER API key rotation"""
        try:
            logger.info(f"üîç [DEBUG] get_channel_videos_safe called with: {channel_id}")
            
            # If it's a custom URL, resolve to actual channel ID first with rotation
            if channel_id.startswith('@'):
                logger.info(f"üîÑ [DEBUG] Resolving custom URL to channel ID: {channel_id}")
                resolved_id = self.resolve_custom_url_to_channel_id(channel_id)
                if resolved_id:
                    channel_id = resolved_id
                    logger.info(f"‚úÖ [DEBUG] Resolved to channel ID: {channel_id}")
                else:
                    logger.error(f"‚ùå [DEBUG] Failed to resolve custom URL: {channel_id}")
                    return {'recent_videos': [], 'video_stats': {'items': []}, 'total_videos_fetched': 0}
            
            # Try all API keys for video fetching
            for key_index, current_key in enumerate(self.youtube_keys):
                try:
                    logger.info(f"üîë [DEBUG] Using API key {key_index + 1} for channel: {channel_id}")
                    
                    # Get uploads playlist
                    channel_url = f"{self.base_url}/channels"
                    channel_params = {
                        'part': 'contentDetails',
                        'id': channel_id,
                        'key': current_key
                    }
                    
                    channel_response = self.session.get(channel_url, params=channel_params, timeout=30)
                    channel_data = channel_response.json()
                    
                    if 'error' in channel_data:
                        error_message = channel_data['error'].get('message', 'Unknown error')
                        if any(keyword in error_message.lower() for keyword in ['quota', 'exceeded', 'disabled', 'forbidden', 'invalid', 'key']):
                            logger.warning(f"‚ùå [DEBUG] Channel API key {key_index + 1} failed: {error_message}")
                            continue  # Try next key
                        else:
                            logger.error(f"‚ùå [DEBUG] Channel API error: {error_message}")
                            break
                    
                    if not channel_data.get('items'):
                        logger.error(f"‚ùå [DEBUG] No channel items found for ID: {channel_id} with key {key_index + 1}")
                        continue  # Try next key
                    
                    # Get the uploads playlist ID
                    uploads_playlist = channel_data['items'][0]['contentDetails']['relatedPlaylists']['uploads']
                    logger.info(f"‚úÖ [DEBUG] Found uploads playlist: {uploads_playlist}")
                    
                    # Get ALL video IDs from the uploads playlist
                    all_video_ids = []
                    next_page_token = None
                    
                    logger.info("üìπ [DEBUG] Fetching video IDs from uploads playlist...")
                    while True:
                        videos_url = f"{self.base_url}/playlistItems"
                        videos_params = {
                            'part': 'contentDetails',
                            'playlistId': uploads_playlist,
                            'maxResults': 50,
                            'key': current_key
                        }
                        
                        if next_page_token:
                            videos_params['pageToken'] = next_page_token
                        
                        videos_response = self.session.get(videos_url, params=videos_params, timeout=30)
                        playlist_data = videos_response.json()
                        
                        if 'error' in playlist_data:
                            error_message = playlist_data['error'].get('message', 'Unknown error')
                            if any(keyword in error_message.lower() for keyword in ['quota', 'exceeded', 'disabled', 'forbidden', 'invalid', 'key']):
                                logger.warning(f"‚ùå [DEBUG] Playlist API key {key_index + 1} failed: {error_message}")
                                break  # Break and try next key
                            else:
                                logger.error(f"‚ùå [DEBUG] Playlist API error: {error_message}")
                                break
                        
                        # Extract video IDs
                        playlist_items = playlist_data.get('items', [])
                        for item in playlist_items:
                            video_id = item['contentDetails']['videoId']
                            all_video_ids.append(video_id)
                        
                        logger.info(f"üìπ [DEBUG] Collected {len(all_video_ids)} video IDs so far...")
                        
                        next_page_token = playlist_data.get('nextPageToken')
                        if not next_page_token or len(all_video_ids) >= 200:
                            break
                    
                    logger.info(f"‚úÖ [DEBUG] Total video IDs collected: {len(all_video_ids)}")
                    
                    # Get detailed stats for ALL videos
                    if all_video_ids:
                        logger.info(f"üîç [DEBUG] Fetching details for {len(all_video_ids)} videos...")
                        video_stats = self.get_video_details_safe(all_video_ids, key_index)
                        
                        if video_stats and 'items' in video_stats:
                            # Sort videos by view count (highest first)
                            sorted_videos = sorted(
                                video_stats['items'],
                                key=lambda x: int(x.get('statistics', {}).get('viewCount', 0)),
                                reverse=True
                            )
                            
                            # Log top videos for debugging
                            logger.info("=== [DEBUG] TOP VIDEOS BY VIEWS ===")
                            for i, video in enumerate(sorted_videos[:3]):
                                views = video.get('statistics', {}).get('viewCount', 0)
                                title = video.get('snippet', {}).get('title', 'Unknown')[:60]
                                likes = video.get('statistics', {}).get('likeCount', 0)
                                logger.info(f"#{i+1}: {views} views, {likes} likes - '{title}...'")
                            
                            return {
                                'recent_videos': sorted_videos[:max_results],
                                'video_stats': {'items': sorted_videos},
                                'total_videos_fetched': len(sorted_videos)
                            }
                        else:
                            logger.error(f"‚ùå [DEBUG] No video details retrieved with key {key_index + 1}")
                            continue  # Try next key
                    else:
                        logger.warning(f"‚ùå [DEBUG] No video IDs found for channel {channel_id} with key {key_index + 1}")
                        continue  # Try next key
                        
                except requests.exceptions.RequestException as e:
                    logger.warning(f"‚ùå [DEBUG] Network error with key {key_index + 1}: {str(e)}")
                    continue
                except Exception as e:
                    logger.warning(f"‚ùå [DEBUG] Error with key {key_index + 1}: {str(e)}")
                    continue
            
            logger.error(f"‚ùå [DEBUG] All API keys exhausted for video fetching")
            return {'recent_videos': [], 'video_stats': {'items': []}, 'total_videos_fetched': 0}
                
        except Exception as e:
            logger.error(f"‚ùå [DEBUG] Error in get_channel_videos_safe: {str(e)}")
            return {'recent_videos': [], 'video_stats': {'items': []}, 'total_videos_fetched': 0}

    def get_video_details_safe(self, video_ids: List[str], key_index: int = 0) -> Dict:
        """FIXED: Get video details with API key rotation"""
        try:
            all_items = []
            batch_size = 20
            
            for i in range(0, len(video_ids), batch_size):
                batch_ids = video_ids[i:i+batch_size]
                
                # Try all API keys for this batch
                batch_success = False
                for current_key_index, current_key in enumerate(self.youtube_keys):
                    try:
                        logger.info(f"üîë Using YouTube API key {current_key_index + 1} for video details batch")
                        
                        url = f"{self.base_url}/videos"
                        params = {
                            'part': 'statistics,snippet,contentDetails,status',
                            'id': ','.join(batch_ids),
                            'key': current_key
                        }
                        
                        response = self.session.get(url, params=params, timeout=30)
                        batch_data = response.json()
                        
                        # Check for API key errors
                        if 'error' in batch_data:
                            error_message = batch_data['error'].get('message', 'Unknown error')
                            if any(keyword in error_message.lower() for keyword in ['quota', 'exceeded', 'disabled', 'forbidden', 'invalid', 'key']):
                                logger.warning(f"‚ùå YouTube API key {current_key_index + 1} failed for video details: {error_message}")
                                continue  # Try next key
                            else:
                                logger.error(f"‚ùå Video details API error: {error_message}")
                                break
                        
                        # Process thumbnails for each video
                        for video in batch_data.get('items', []):
                            snippet = video.get('snippet', {})
                            thumbnails = snippet.get('thumbnails', {})
                            
                            # Extract thumbnail URL (prefer high quality)
                            thumbnail_url = None
                            if thumbnails.get('maxres'):
                                thumbnail_url = thumbnails['maxres']['url']
                            elif thumbnails.get('high'):
                                thumbnail_url = thumbnails['high']['url']
                            elif thumbnails.get('medium'):
                                thumbnail_url = thumbnails['medium']['url']
                            elif thumbnails.get('default'):
                                thumbnail_url = thumbnails['default']['url']
                            
                            # Add thumbnail URL to video data
                            if thumbnail_url:
                                if 'thumbnail_url' not in video:
                                    video['thumbnail_url'] = thumbnail_url
                                snippet['thumbnail_url'] = thumbnail_url
                        
                        all_items.extend(batch_data.get('items', []))
                        batch_success = True
                        break  # Success, break key rotation loop for this batch
                        
                    except requests.exceptions.RequestException as e:
                        logger.warning(f"‚ùå Network error with key {current_key_index + 1}: {str(e)}")
                        continue
                    except Exception as e:
                        logger.warning(f"‚ùå Error with key {current_key_index + 1}: {str(e)}")
                        continue
                
                if not batch_success:
                    logger.error(f"‚ùå All API keys failed for video batch {i//batch_size + 1}")
                
                if i + batch_size < len(video_ids):
                    time.sleep(0.5)  # Rate limiting
            
            return {'items': all_items}
        except Exception as e:
            logger.error(f"Error fetching video details: {str(e)}")
            return {'items': []}


    # ==================== METRIC CALCULATION METHODS ====================
    
    def calculate_comprehensive_engagement(self, videos_data: Dict) -> Dict:
        """Calculate engagement metrics with PROPER top video identification"""
        if not videos_data or 'video_stats' not in videos_data:
            return self.get_default_engagement_metrics()
        
        items = videos_data['video_stats'].get('items', [])
        if not items:
            return self.get_default_engagement_metrics()
        
        total_views = 0
        total_likes = 0
        total_comments = 0
        engagement_rates = []
        view_counts = []
        like_counts = []
        comment_counts = []
        durations = []
        
        for video in items:
            stats = video.get('statistics', {})
            views = int(stats.get('viewCount', 0))
            likes = int(stats.get('likeCount', 0))
            comments = int(stats.get('commentCount', 0))
            
            total_views += views
            total_likes += likes
            total_comments += comments
            
            view_counts.append(views)
            like_counts.append(likes)
            comment_counts.append(comments)
            
            # Calculate duration
            duration = video.get('contentDetails', {}).get('duration', 'PT0S')
            duration_seconds = self.parse_duration(duration)
            durations.append(duration_seconds)
            
            # Engagement rate calculation
            if views > 0:
                engagement_rate = ((likes + comments) / views) * 100
                engagement_rates.append(engagement_rate)
        
        # üî• FIXED: Since videos are already sorted by views, first video is the top performer
        num_videos = len(items)
        avg_engagement = np.mean(engagement_rates) if engagement_rates else 0
        engagement_health = self.assess_engagement_health_enhanced(avg_engagement, num_videos)
        
        engagement_consistency = self.calculate_consistency(engagement_rates) if engagement_rates else 50
        performance_consistency = self.calculate_consistency(view_counts) if view_counts else 50
        
        # For small channels, be more generous with consistency
        if num_videos < 3:
            engagement_consistency = max(engagement_consistency, 60)
            performance_consistency = max(performance_consistency, 60)
        
        # üî• CORRECT: Top video is now the first one (already sorted by views)
        top_video = items[0] if items else None
        
        # Extract top video details
        top_video_thumbnail = None
        top_video_id = None
        
        if top_video:
            snippet = top_video.get('snippet', {})
            top_video_thumbnail = top_video.get('thumbnail_url') or snippet.get('thumbnail_url')
            top_video_id = top_video.get('id')
        
        engagement_data = {
            'total_recent_views': total_views,
            'total_recent_likes': total_likes,
            'total_recent_comments': total_comments,
            'total_engagement': total_likes + total_comments,
            
            'avg_engagement_rate': round(avg_engagement, 2),
            'engagement_consistency': engagement_consistency,
            'engagement_health': engagement_health,
            
            'avg_views_per_video': round(total_views / num_videos, 1) if num_videos > 0 else 0,
            'avg_likes_per_video': round(total_likes / num_videos, 1) if num_videos > 0 else 0,
            'avg_comments_per_video': round(total_comments / num_videos, 1) if num_videos > 0 else 0,
            
            'views_std_dev': round(np.std(view_counts) if view_counts else 0, 1),
            'performance_consistency': performance_consistency,
            
            # üî• THESE ARE NOW CORRECT - using the actual top video by views
            'top_performing_video_views': int(top_video.get('statistics', {}).get('viewCount', 0)) if top_video else 0,
            'top_performing_video_likes': int(top_video.get('statistics', {}).get('likeCount', 0)) if top_video else 0,
            'top_performing_video_title': top_video.get('snippet', {}).get('title', '') if top_video else '',
            'top_performing_video_thumbnail': top_video_thumbnail,
            'top_performing_video_id': top_video_id,
            
            'videos_analyzed': num_videos
        }
        
        # üî• DEBUG: Verify we have the correct top video
        logger.info("=== TOP VIDEO VERIFICATION ===")
        logger.info(f"Top video views: {engagement_data['top_performing_video_views']}")
        logger.info(f"Top video title: {engagement_data['top_performing_video_title'][:50]}...")
        logger.info(f"Total videos analyzed: {engagement_data['videos_analyzed']}")
        
        return engagement_data

    def calculate_performance_metrics_enhanced(self, videos_data: Dict, engagement_metrics: Dict) -> Dict:
        """Calculate enhanced video performance metrics"""
        if not videos_data or 'video_stats' not in videos_data:
            return self.get_default_performance_metrics()
        
        items = videos_data['video_stats'].get('items', [])
        if not items:
            return self.get_default_performance_metrics()
        
        durations = []
        performance_scores = []
        retention_estimates = []
        
        for video in items:
            stats = video.get('statistics', {})
            views = int(stats.get('viewCount', 0))
            likes = int(stats.get('likeCount', 0))
            comments = int(stats.get('commentCount', 0))
            
            # Calculate duration
            duration = video.get('contentDetails', {}).get('duration', 'PT0S')
            duration_seconds = self.parse_duration(duration)
            durations.append(duration_seconds)
            
            # Enhanced performance score
            if views > 0:
                engagement_score = (likes + comments) / views
                performance_score = views * (1 + engagement_score)
                performance_scores.append(performance_score)
                
                # Estimate retention based on engagement
                retention_estimate = min(engagement_score * 200, 95)
                retention_estimates.append(retention_estimate)
        
        # Calculate enhanced metrics
        avg_duration = np.mean(durations) if durations else 0
        avg_performance = np.mean(performance_scores) if performance_scores else 0
        avg_retention = np.mean(retention_estimates) if retention_estimates else 0
        
        performance_data = {
            'avg_duration_seconds': round(avg_duration, 1),
            'avg_performance_score': round(avg_performance, 1),
            'optimal_video_length': self.determine_optimal_length_enhanced(durations, performance_scores),
            'duration_consistency': self.calculate_consistency(durations),
            'performance_trend': self.analyze_performance_trend_enhanced(performance_scores),
            'estimated_retention_rate': round(avg_retention, 1),
            'content_velocity_score': self.calculate_content_velocity(items)
        }
        
        logger.info("=== PERFORMANCE METRICS DEBUG ===")
        logger.info(f"Optimal video length: {performance_data['optimal_video_length']}")
        logger.info(f"Performance trend: {performance_data['performance_trend']}")
        logger.info(f"Avg duration: {performance_data['avg_duration_seconds']}")
        
        return performance_data

    # ==================== HELPER METHODS ====================
    
    def calculate_channel_age(self, published_at: str) -> int:
        """Calculate channel age in days"""
        try:
            if not published_at:
                return 0
            
            publish_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            current_date = datetime.now(timezone.utc)
            age_days = (current_date - publish_date).days
            return max(age_days, 1)
        except Exception:
            return 0

    def calculate_consistency(self, values: List[float]) -> float:
        """FIXED: More realistic consistency calculation"""
        if not values or len(values) < 2:
            return 75  # Reasonable default
        
        try:
            # For active channels, be more generous
            if len(values) >= 3:
                # Use coefficient of variation but with realistic scaling
                mean_val = np.mean(values)
                if mean_val == 0:
                    return 65  # Active but low numbers
                
                std_dev = np.std(values)
                cv = (std_dev / mean_val) * 100
                
                # Realistic consistency scaling for YouTube
                if cv < 50:
                    consistency = 85 - (cv / 2)  # Good consistency
                elif cv < 100:
                    consistency = 70 - (cv / 4)   # Moderate consistency
                else:
                    consistency = 40 - (cv / 10)  # Poor consistency
                
                return max(20, min(consistency, 95))
            else:
                return 70  # Reasonable for small sample
                
        except Exception:
            return 65  # Fallback to reasonable default

    def parse_duration(self, duration: str) -> int:
        """Parse ISO 8601 duration to seconds"""
        import re
        match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration)
        if not match:
            return 0
        
        hours = int(match.group(1)) if match.group(1) else 0
        minutes = int(match.group(2)) if match.group(2) else 0
        seconds = int(match.group(3)) if match.group(3) else 0
        
        return hours * 3600 + minutes * 60 + seconds

    def determine_optimal_length_enhanced(self, durations: List[float], performances: List[float]) -> str:
        """Enhanced optimal length determination"""
        if not durations or not performances:
            return 'Medium (8-15 minutes)'
        
        # Weighted analysis based on performance
        weighted_durations = []
        for duration, performance in zip(durations, performances):
            weight = performance / max(performances) if max(performances) > 0 else 1
            weighted_durations.extend([duration] * int(weight * 10))
        
        if not weighted_durations:
            avg_duration = np.mean(durations)
        else:
            avg_duration = np.mean(weighted_durations)
        
        if avg_duration < 180:
            return 'Short (under 3 minutes)'
        elif avg_duration < 480:
            return 'Medium-Short (3-8 minutes)'
        elif avg_duration < 900:
            return 'Medium (8-15 minutes)'
        elif avg_duration < 1800:
            return 'Long (15-30 minutes)'
        else:
            return 'Very Long (30+ minutes)'

    def analyze_performance_trend_enhanced(self, performances: List[float]) -> str:
        """COMPLETELY FIXED: Realistic performance trend analysis"""
        if not performances or len(performances) < 5:
            return 'Insufficient Data'
        
        # Use only recent performances (last 10 videos max)
        recent_performances = performances[:min(10, len(performances))]
        
        if len(recent_performances) < 5:
            return 'Insufficient Data'
        
        # Calculate simple moving average trend
        if len(recent_performances) >= 5:
            # Split into two equal halves
            half_point = len(recent_performances) // 2
            first_half = recent_performances[half_point:]  # Older videos
            second_half = recent_performances[:half_point]  # Newer videos
            
            first_avg = np.mean(first_half) if first_half else 0
            second_avg = np.mean(second_half) if second_half else 0
            
            if first_avg == 0:
                return 'Stable'
            
            # Calculate percentage change (newer vs older)
            percentage_change = ((second_avg - first_avg) / first_avg) * 100
            
            # REALISTIC YouTube thresholds
            if percentage_change > 100:
                return 'Explosive Growth'
            elif percentage_change > 50:
                return 'Rapid Growth'
            elif percentage_change > 20:
                return 'Growing'
            elif percentage_change > 5:
                return 'Slow Growth'
            elif percentage_change > -20:  # Much wider stable range
                return 'Stable'
            elif percentage_change > -50:
                return 'Declining'
            else:
                return 'Rapid Decline'
        
        return 'Stable'  # Default to stable if we can't calculate

    def assess_engagement_health_enhanced(self, engagement_rate: float, video_count: int, category: str = 'general') -> str:
        """Enhanced engagement assessment with category-specific thresholds"""
        
        # Category-specific thresholds
        category_thresholds = {
            'gaming': {'excellent': 6, 'good': 4, 'average': 2.5, 'poor': 1.5},
            'tech': {'excellent': 8, 'good': 5, 'average': 3, 'poor': 1.5},
            'education': {'excellent': 7, 'good': 4.5, 'average': 2.5, 'poor': 1},
            'entertainment': {'excellent': 5, 'good': 3, 'average': 1.5, 'poor': 0.5},
            'general': {'excellent': 8, 'good': 5, 'average': 3, 'poor': 1}
        }
        
        thresholds = category_thresholds.get(category, category_thresholds['general'])
        
        if engagement_rate >= thresholds['excellent']:
            return 'Excellent'
        elif engagement_rate >= thresholds['good']:
            return 'Good'
        elif engagement_rate >= thresholds['average']:
            return 'Average'
        else:
            return 'Needs Work'

    def calculate_content_velocity(self, videos: List[Dict]) -> float:
        """Calculate content velocity score"""
        if len(videos) < 2:
            return 0
        
        try:
            dates = []
            for video in videos:
                publish_time = video['snippet']['publishedAt']
                publish_date = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
                dates.append(publish_date)
            
            dates.sort()
            total_days = (dates[-1] - dates[0]).days
            
            if total_days == 0:
                return 100
            
            videos_per_week = (len(videos) / total_days) * 7
            velocity = min(videos_per_week * 10, 100)
            
            return round(velocity, 1)
        except:
            return 0

    # ==================== DEFAULT METRICS ====================
    
    def get_default_engagement_metrics(self) -> Dict:
        """Return default engagement metrics"""
        return {
            'total_recent_views': 0,
            'total_recent_likes': 0,
            'total_recent_comments': 0,
            'total_engagement': 0,
            'avg_engagement_rate': 0,
            'engagement_consistency': 0,
            'engagement_health': 'Unknown',
            'avg_views_per_video': 0,
            'avg_likes_per_video': 0,
            'avg_comments_per_video': 0,
            'views_std_dev': 0,
            'performance_consistency': 0,
            'top_performing_video_views': 0,
            'top_performing_video_likes': 0,
            'top_performing_video_title': '',
            'videos_analyzed': 0
        }

    def get_default_performance_metrics(self) -> Dict:
        """Return default performance metrics"""
        return {
            'avg_duration_seconds': 0,
            'avg_performance_score': 0,
            'optimal_video_length': 'Unknown',
            'duration_consistency': 0,
            'performance_trend': 'Unknown',
            'estimated_retention_rate': 0,
            'content_velocity_score': 0
        }

    # ==================== AI METHODS (UPDATED) ====================
    
    def analyze_content_strategy_enhanced(self, videos_data: Dict, channel_data: Dict) -> Dict:
        """Enhanced content strategy analysis"""
        if not videos_data or 'video_stats' not in videos_data:
            return self.get_default_content_analysis()
        
        items = videos_data['video_stats'].get('items', [])
        if not items:
            return self.get_default_content_analysis()
        
        # Enhanced content categorization
        categories = self.categorize_content_ai_enhanced(items)
        
        # Advanced publishing analysis
        frequency_analysis = self.analyze_publishing_pattern_enhanced(items)
        
        # Title and thumbnail analysis
        title_analysis = self.analyze_titles_ai_enhanced(items)
        
        # Content gaps with AI insights
        content_gaps = self.identify_content_gaps_ai(categories, channel_data)
        
        return {
            'content_categories': categories,
            'publishing_frequency': frequency_analysis['frequency'],
            'publishing_consistency': frequency_analysis['consistency'],
            'title_optimization': title_analysis,
            'content_gaps': content_gaps,
            'trending_alignment_score': self.assess_trending_alignment_ai(items),
            'content_diversity_score': self.calculate_diversity_score_enhanced(categories),
            'content_freshness_score': self.assess_content_freshness(items),
            'total_videos_analyzed': len(items)
        }

    def get_default_content_analysis(self) -> Dict:
        """Return default content analysis"""
        return {
            'content_categories': {},
            'publishing_frequency': 'Unknown',
            'publishing_consistency': 0,
            'title_optimization': {},
            'content_gaps': [],
            'trending_alignment_score': 0,
            'content_diversity_score': 0,
            'content_freshness_score': 0,
            'total_videos_analyzed': 0
        }

    def calculate_ai_enhanced_metrics(self, channel_data: Dict, engagement_metrics: Dict, 
                                    content_analysis: Dict, performance_metrics: Dict) -> Dict:
        """PROPERLY calculate AI metrics based on actual channel performance"""
        
        # Get actual performance data
        subscribers = channel_data.get('subscribers', 0)
        engagement_rate = engagement_metrics.get('avg_engagement_rate', 0)
        top_video_views = engagement_metrics.get('top_performing_video_views', 0)
        consistency = engagement_metrics.get('performance_consistency', 0)
        content_diversity = content_analysis.get('content_diversity_score', 0)
        performance_trend = performance_metrics.get('performance_trend', 'Unknown')
        
        # Calculate viral ratio
        viral_ratio = top_video_views / max(subscribers, 1)
        
        # DYNAMIC calculations based on actual performance
        channel_scale = self.assess_channel_scale(channel_data)
        
        # Channel Health Score (0-100) - Scale appropriately
        if channel_scale == "mega":
            # Mega channels have different benchmarks
            health_score = min(
                min(engagement_rate * 10, 30) +  # Lower engagement expected
                min(consistency * 0.8, 30) +     # Consistency matters more
                min(content_diversity * 0.4, 20) +
                min(viral_ratio * 0.1, 20),      # Viral ratio less impactful
                100
            )
        elif channel_scale == "small":
            # Small channels can have higher engagement
            health_score = min(
                min(engagement_rate * 8, 40) +
                min(consistency * 0.6, 25) +
                min(content_diversity * 0.5, 20) +
                min(viral_ratio * 3, 15),        # Viral ratio very impactful
                100
            )
        else:
            # Standard calculation
            health_score = min(
                min(engagement_rate * 6, 35) +
                min(consistency * 0.5, 25) +
                min(content_diversity * 0.4, 20) +
                min(viral_ratio * 2, 20),
                100
            )
        
        # Content Quality Score - Based on actual performance
        content_quality = min(
            # Engagement quality (0-40 points)
            min(engagement_rate * 7, 40) +
            # Performance consistency (0-30 points)
            min(consistency * 0.4, 30) +
            # Content diversity (0-20 points)
            min(content_diversity * 0.3, 20) +
            # Trend bonus (0-10 points)
            (10 if performance_trend in ['Explosive Growth', 'Rapid Growth'] else 
            5 if performance_trend in ['Growing', 'Stable'] else 0),
            100
        )
        
        # Growth Potential - Realistic assessment
        if channel_scale == "mega":
            if engagement_rate >= 2 and consistency >= 70:
                growth_potential = "Medium"
            elif engagement_rate >= 1.5:
                growth_potential = "Low-Medium"
            else:
                growth_potential = "Low"
        else:
            if viral_ratio > 20 or engagement_rate > 8:
                growth_potential = "High"
            elif viral_ratio > 10 or engagement_rate > 6:
                growth_potential = "Medium-High"
            elif viral_ratio > 5 or engagement_rate > 4:
                growth_potential = "Medium"
            elif engagement_rate > 2:
                growth_potential = "Low-Medium"
            else:
                growth_potential = "Low"
        
        # Performance Tier - Based on actual scores
        if health_score >= 80 and engagement_rate >= (6 if channel_scale != "mega" else 2):
            performance_tier = "Excellent"
        elif health_score >= 65 and engagement_rate >= (4 if channel_scale != "mega" else 1.5):
            performance_tier = "Good"
        elif health_score >= 50:
            performance_tier = "Average"
        else:
            performance_tier = "Needs Improvement"
        
        logger.info(f"üîç AI METRICS CALCULATION:")
        logger.info(f"   Scale: {channel_scale}, Subs: {subscribers}")
        logger.info(f"   Health: {health_score}, Quality: {content_quality}")
        logger.info(f"   Growth: {growth_potential}, Viral: {viral_ratio:.1f}X")
        
        return {
            'channel_health_score': round(health_score),
            'content_quality_score': round(content_quality),
            'growth_potential': growth_potential,
            'performance_tier': performance_tier,
            'viral_ratio': round(viral_ratio, 1),
            'engagement_health': self.assess_engagement_health_scaled(engagement_rate, channel_scale),
            'audience_loyalty_score': round(min((engagement_rate * 1.2) + (consistency * 0.3), 100)),
            'algorithm_favorability': round(min((consistency * 0.4) + (content_diversity * 0.3) + (engagement_rate * 0.3), 100))
        }
    
    def calculate_metrics_with_groq(self, channel_data: Dict, engagement_metrics: Dict,
                                content_analysis: Dict, performance_metrics: Dict) -> Optional[Dict]:
        """Calculate all metrics using Groq AI - FIXED to remove hardcoded values"""
        for key_index, api_key in enumerate(self.groq_keys):
            try:
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
                
                # Prepare comprehensive data for AI analysis
                analysis_data = {
                    'channel_scale': self.assess_channel_scale(channel_data),
                    'subscribers': channel_data.get('subscribers', 0),
                    'total_views': channel_data.get('total_views', 0),
                    'total_videos': channel_data.get('total_videos', 0),
                    'channel_age_days': channel_data.get('channel_age_days', 0),
                    'engagement_rate': engagement_metrics.get('avg_engagement_rate', 0),
                    'top_video_views': engagement_metrics.get('top_performing_video_views', 0),
                    'performance_consistency': engagement_metrics.get('performance_consistency', 0),
                    'content_diversity': content_analysis.get('content_diversity_score', 0),
                    'optimal_length': performance_metrics.get('optimal_video_length', 'Unknown'),
                    'performance_trend': performance_metrics.get('performance_trend', 'Unknown'),
                    'content_velocity': performance_metrics.get('content_velocity_score', 0)
                }
                
                # Calculate viral ratio for context
                viral_ratio = analysis_data['top_video_views'] / max(analysis_data['subscribers'], 1)
                
                prompt = f"""
                As a YouTube analytics expert, analyze this channel data and calculate PRECISE, REAL metrics based on ACTUAL performance.
                DO NOT use any hardcoded values - calculate everything based on the actual data provided.

                CHANNEL DATA:
                - Scale: {analysis_data['channel_scale']}
                - Subscribers: {analysis_data['subscribers']:,}
                - Total Views: {analysis_data['total_views']:,}
                - Total Videos: {analysis_data['total_videos']}
                - Channel Age: {analysis_data['channel_age_days']} days
                - Engagement Rate: {analysis_data['engagement_rate']}%
                - Top Video Views: {analysis_data['top_video_views']:,}
                - Performance Consistency: {analysis_data['performance_consistency']}/100
                - Content Diversity: {analysis_data['content_diversity']}/100
                - Optimal Video Length: {analysis_data['optimal_length']}
                - Performance Trend: {analysis_data['performance_trend']}
                - Content Velocity: {analysis_data['content_velocity']}/100
                - Viral Ratio: {viral_ratio:.1f}X

                Calculate REAL metrics based on this actual performance:

                Channel Health Score (0-100):
                - Base on engagement rate, consistency, and content diversity
                - Mega channels (>100M): Different benchmarks (lower engagement expected)
                - Small channels (<100K): Different benchmarks (higher growth potential)

                Content Quality Score (0-100):
                - Based on engagement quality, content diversity, and performance trend
                - Consider if content is resonating with audience

                Growth Potential (High/Medium-High/Medium/Low-Medium/Low):
                - Based on viral ratio, engagement rate, and performance trend
                - Viral ratio > 20 = High, >10 = Medium-High, >5 = Medium, etc.

                Performance Tier (Excellent/Good/Average/Needs Improvement):
                - Based on overall channel health and engagement metrics

                IMPORTANT: 
                - Calculate REAL scores based on the ACTUAL data above
                - DO NOT use any hardcoded or example values
                - A channel with low subscribers (eg: 1k) should NOT get the same scores as a channel with high subscribers (eg: 1M)
                - Scale expectations appropriately for channel size
                - If engagement rate is 1.4%, health score should reflect this realistically

                Return ONLY valid JSON with calculated values:
                {{
                    "channel_health_score": [calculated_value],
                    "content_quality_score": [calculated_value], 
                    "growth_potential": "[calculated_value]",
                    "performance_tier": "[calculated_value]",
                    "viral_ratio": [actual_viral_ratio],
                    "engagement_health": "[calculated_value]",
                    "audience_loyalty_score": [calculated_value],
                    "algorithm_favorability": [calculated_value]
                }}
                """
                
                payload = {
                    "messages": [{"role": "user", "content": prompt}],
                    "model": "llama-3.1-8b-instant",
                    "temperature": 0.3,
                    "max_tokens": 1000,
                    "stream": False
                }
                
                response = requests.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    
                    # Parse JSON response
                    import re
                    json_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        ai_metrics = json.loads(json_str)
                        
                        # üî• CRITICAL: Validate that we're not getting hardcoded values
                        if self.validate_ai_metrics_not_hardcoded(ai_metrics):
                            logger.info(f"‚úÖ AI metrics calculated successfully with key {key_index + 1}")
                            return ai_metrics
                        else:
                            logger.warning(f"‚ùå AI returned hardcoded values, using mathematical fallback")
                            return None
                
            except Exception as e:
                logger.warning(f"‚ùå Groq key {key_index + 1} failed for metrics: {str(e)}")
                continue
        
        return None
    
    def calculate_enhanced_mathematical_metrics(self, channel_data: Dict, engagement_metrics: Dict,
                                            content_analysis: Dict, performance_metrics: Dict) -> Dict:
        """FIXED: Proper mathematical metrics that actually use real channel data"""
        
        # üî• FIX: Use CORRECT data sources
        subscribers = channel_data.get('subscribers', 0)
        engagement_rate = engagement_metrics.get('avg_engagement_rate', 0)
        top_video_views = engagement_metrics.get('top_performing_video_views', 0)  # FIXED SOURCE
        consistency = engagement_metrics.get('performance_consistency', 0)
        content_diversity = content_analysis.get('content_diversity_score', 0)
        content_velocity = performance_metrics.get('content_velocity_score', 0)
        performance_trend = performance_metrics.get('performance_trend', 'Unknown')
        
        # üî• CRITICAL FIX: Consistent viral ratio calculation
        if subscribers > 0 and top_video_views > 0:
            viral_ratio = top_video_views / subscribers
        else:
            viral_ratio = 0
        
        logger.info(f"üîç VIRAL RATIO CALCULATION: {top_video_views} / {subscribers} = {viral_ratio:.1f}X")
        
        # üî• DYNAMIC CALCULATIONS BASED ON ACTUAL PERFORMANCE
        
        # Channel Health Score (0-100) - Based on actual metrics
        health_score = min(
            # Engagement component (0-40 points)
            min(engagement_rate * 6, 40) +
            # Consistency component (0-25 points)  
            min(consistency * 0.4, 25) +
            # Content quality component (0-20 points)
            min(content_diversity * 0.3, 20) +
            # Growth potential component (0-15 points)
            min(viral_ratio * 3, 15),
            100
        )
        
        # Content Quality Score (0-100) - Based on actual performance
        content_quality = min(
            # Engagement quality (0-50 points)
            min(engagement_rate * 8, 50) +
            # Diversity and consistency (0-30 points)
            min(content_diversity * 0.4 + consistency * 0.2, 30) +
            # Performance bonus (0-20 points)
            (20 if performance_trend in ['Explosive Growth', 'Rapid Growth'] else 
            10 if performance_trend in ['Growing', 'Stable'] else 0),
            100
        )
        
        # Growth Potential - Based on actual viral performance and engagement
        if viral_ratio > 20 or engagement_rate > 8:
            growth_potential = "High"
        elif viral_ratio > 10 or engagement_rate > 6:
            growth_potential = "Medium-High"
        elif viral_ratio > 5 or engagement_rate > 4:
            growth_potential = "Medium"
        elif engagement_rate > 2:
            growth_potential = "Low-Medium"
        else:
            growth_potential = "Low"
        
        # Performance Tier - Based on actual scores
        if health_score >= 80 and engagement_rate >= 6:
            performance_tier = "Excellent"
        elif health_score >= 65 and engagement_rate >= 4:
            performance_tier = "Good"
        elif health_score >= 50:
            performance_tier = "Average"
        else:
            performance_tier = "Needs Improvement"
        
        # Engagement Health - Based on actual rate
        if engagement_rate >= 8:
            engagement_health = "Excellent"
        elif engagement_rate >= 5:
            engagement_health = "Good" 
        elif engagement_rate >= 3:
            engagement_health = "Average"
        else:
            engagement_health = "Needs Work"
        
        return {
            'channel_health_score': round(health_score),
            'content_quality_score': round(content_quality),
            'growth_potential': growth_potential,
            'performance_tier': performance_tier,
            'viral_ratio': round(viral_ratio, 1),
            'engagement_health': engagement_health,
            'audience_loyalty_score': round(min((engagement_rate * 1.2) + (consistency * 0.3), 100)),
            'algorithm_favorability': round(min((consistency * 0.4) + (content_velocity * 0.3) + (engagement_rate * 0.3), 100))
        }
    
    def unify_data_sources(self, analysis: Dict) -> Dict:
        """Ensure all data sources are consistent and realistic"""
        
        # Get actual performance data
        subscribers = analysis['channel_metrics'].get('subscribers', 0)
        engagement_rate = analysis['engagement_metrics'].get('avg_engagement_rate', 0)
        top_video_views = analysis['engagement_metrics'].get('top_performing_video_views', 0)
        
        # Calculate consistent viral ratio
        viral_ratio = top_video_views / max(subscribers, 1)
        
        # Update AI metrics to ensure consistency
        if 'ai_enhanced_metrics' in analysis:
            analysis['ai_enhanced_metrics']['viral_ratio'] = round(viral_ratio, 1)
            
            # Ensure scores make sense for the channel size
            channel_scale = self.assess_channel_scale(analysis['channel_metrics'])
            
            # Small channels with low engagement shouldn't have high scores
            if channel_scale in ['nano', 'micro'] and engagement_rate < 3:
                current_health = analysis['ai_enhanced_metrics'].get('channel_health_score', 0)
                if current_health > 70:  # Too high for small channel with low engagement
                    analysis['ai_enhanced_metrics']['channel_health_score'] = max(30, min(60, current_health))
        
        logger.info(f"üîó DATA UNIFICATION: Viral ratio {viral_ratio:.1f}X, Engagement {engagement_rate}%")
        
        return analysis

    def assess_channel_scale(self, channel_data: Dict) -> str:
        """More granular channel scale assessment"""
        subscribers = channel_data.get('subscribers', 0)
        
        if subscribers >= 10000000:  # 10M+
            return "mega"
        elif subscribers >= 1000000:  # 1M+
            return "large" 
        elif subscribers >= 100000:   # 100K+
            return "medium"
        elif subscribers >= 10000:    # 10K+
            return "small"
        elif subscribers >= 1000:     # 1K+
            return "micro"
        else:
            return "nano"  # <1K subs
    
    def calculate_mega_channel_health(self, engagement_rate: float, consistency: float,
                                    diversity: float, velocity: float, viral_ratio: float) -> float:
        """Health calculation for mega channels (different benchmarks)"""
        # Mega channels have lower engagement rates but massive reach
        engagement_score = min(engagement_rate * 8, 25)  # Different multiplier
        consistency_score = min(consistency * 0.6, 25)
        diversity_score = min(diversity * 0.3, 20)
        velocity_score = min(velocity * 0.2, 15)
        viral_score = min(viral_ratio * 0.5, 15)  # Viral ratio matters less for established channels
        
        return engagement_score + consistency_score + diversity_score + velocity_score + viral_score
    
    def calculate_large_channel_health(self, engagement_rate: float, consistency: float,
                                     diversity: float, velocity: float, viral_ratio: float) -> float:
        """Health calculation for large channels"""
        engagement_score = min(engagement_rate * 6, 25)
        consistency_score = min(consistency * 0.5, 20)
        diversity_score = min(diversity * 0.3, 15)
        velocity_score = min(velocity * 0.2, 10)
        viral_score = min(viral_ratio * 1.0, 10)
        
        return engagement_score + consistency_score + diversity_score + velocity_score + viral_score
    
    def calculate_standard_channel_health(self, engagement_rate: float, consistency: float,
                                    diversity: float, velocity: float, viral_ratio: float) -> float:
        """Health calculation for standard channels"""
        engagement_score = min(engagement_rate * 5, 30)
        consistency_score = min(consistency * 0.4, 25)
        diversity_score = min(diversity * 0.3, 20)
        velocity_score = min(velocity * 0.2, 15)
        viral_score = min(viral_ratio * 2.0, 10)
        
        return engagement_score + consistency_score + diversity_score + velocity_score + viral_score
    
    def assess_growth_potential_scaled(self, engagement_rate: float, viral_ratio: float,
                                     consistency: float, scale: str) -> str:
        """Growth potential assessment that scales appropriately"""
        
        if scale == "mega":
            # Mega channels have different growth dynamics
            if engagement_rate >= 2 and consistency >= 70:
                return "Medium"
            elif engagement_rate >= 1.5:
                return "Low-Medium"
            else:
                return "Low"
                
        elif scale == "large":
            if viral_ratio > 5 or engagement_rate >= 6:
                return "High"
            elif viral_ratio > 2 or engagement_rate >= 4:
                return "Medium-High"
            elif engagement_rate >= 2.5:
                return "Medium"
            else:
                return "Low-Medium"
                
        else:  # medium and small channels
            if viral_ratio > 10 or engagement_rate >= 8:
                return "High"
            elif viral_ratio > 3 or engagement_rate >= 5:
                return "Medium-High" 
            elif engagement_rate >= 3:
                return "Medium"
            elif engagement_rate >= 2:
                return "Low-Medium"
            else:
                return "Low"
            
    def assess_performance_tier_scaled(self, health_score: float, engagement_rate: float,
                                    consistency: float, scale: str) -> str:
        """Performance tier assessment that scales appropriately"""
        
        if scale == "mega":
            if health_score >= 75 and engagement_rate >= 2 and consistency >= 75:
                return "Excellent"
            elif health_score >= 60 and engagement_rate >= 1.5 and consistency >= 65:
                return "Good"
            elif health_score >= 45:
                return "Average"
            else:
                return "Needs Improvement"
                
        else:
            if health_score >= 80 and engagement_rate >= 6 and consistency >= 75:
                return "Excellent"
            elif health_score >= 65 and engagement_rate >= 4 and consistency >= 60:
                return "Good" 
            elif health_score >= 50:
                return "Average"
            else:
                return "Needs Improvement"
    
    def calculate_content_quality_scaled(self, engagement_rate: float, diversity: float,
                                      consistency: float, scale: str) -> float:
        """Content quality score with appropriate scaling"""
        if scale == "mega":
            base_score = (engagement_rate * 4) + (diversity * 0.3) + (consistency * 0.2)
        elif scale == "large":
            base_score = (engagement_rate * 5) + (diversity * 0.4) + (consistency * 0.3)
        else:
            base_score = (engagement_rate * 6) + (diversity * 0.5) + (consistency * 0.4)
        
        return min(base_score, 100)
    
    def assess_engagement_health_scaled(self, engagement_rate: float, scale: str) -> str:
        """Engagement health assessment that scales with channel size"""
        
        if scale == "mega":
            # Mega channels have much lower engagement rates normally
            if engagement_rate >= 3:
                return "Excellent"
            elif engagement_rate >= 2:
                return "Good"
            elif engagement_rate >= 1:
                return "Average"
            else:
                return "Needs Work"
                
        elif scale == "large":
            if engagement_rate >= 6:
                return "Excellent"
            elif engagement_rate >= 4:
                return "Good"
            elif engagement_rate >= 2.5:
                return "Average"
            else:
                return "Needs Work"
                
        else:
            if engagement_rate >= 8:
                return "Excellent"
            elif engagement_rate >= 5:
                return "Good"
            elif engagement_rate >= 3:
                return "Average"
            else:
                return "Needs Work"

    def calculate_loyalty_score(self, engagement_rate: float, consistency: float) -> float:
        """Calculate audience loyalty score"""
        return min((engagement_rate * 1.2) + (consistency * 0.3), 100)
    
    def calculate_algorithm_score(self, consistency: float, velocity: float, engagement_rate: float) -> float:
        """Calculate algorithm favorability score"""
        return min((consistency * 0.4) + (velocity * 0.3) + (engagement_rate * 0.3), 100)
    
    def validate_ai_metrics_not_hardcoded(self, metrics: Dict) -> bool:
        """Strict validation against hardcoded values"""
        # Common hardcoded patterns to reject
        hardcoded_patterns = [
            {'channel_health_score': 85, 'content_quality_score': 92, 'growth_potential': 'High'},
            {'channel_health_score': 92, 'content_quality_score': 85, 'growth_potential': 'High'},
            {'channel_health_score': 88, 'content_quality_score': 90, 'growth_potential': 'High'}
        ]
        
        for pattern in hardcoded_patterns:
            if (metrics.get('channel_health_score') == pattern['channel_health_score'] and
                metrics.get('content_quality_score') == pattern['content_quality_score'] and
                metrics.get('growth_potential') == pattern['growth_potential']):
                logger.error("‚ùå AI returned hardcoded pattern, rejecting")
                return False
        
        # Additional sanity checks
        health_score = metrics.get('channel_health_score', 0)
        quality_score = metrics.get('content_quality_score', 0)
        
        # Scores should be realistic for the data
        if health_score > 80 and quality_score > 85:
            # High scores should only come from high-performing channels
            # Add your logic to verify this makes sense
            pass
            
        return True
    
    def validate_ai_metrics(self, metrics: Dict) -> bool:
        """Validate that AI-generated metrics are reasonable"""
        required_fields = [
            'channel_health_score', 'content_quality_score', 'growth_potential',
            'performance_tier', 'viral_ratio', 'engagement_health'
        ]
        
        # Check required fields
        for field in required_fields:
            if field not in metrics:
                return False
        
        # Validate score ranges
        if not (0 <= metrics.get('channel_health_score', 0) <= 100):
            return False
        if not (0 <= metrics.get('content_quality_score', 0) <= 100):
            return False
        
        # Validate growth potential values
        valid_growth = ['High', 'Medium-High', 'Medium', 'Low-Medium', 'Low']
        if metrics.get('growth_potential') not in valid_growth:
            return False
        
        return True
    
    def _calculate_viral_bonus(self, viral_ratio: float) -> float:
        """Calculate bonus based on viral performance"""
        if viral_ratio > 50:
            return 12.0  # Massive viral success
        elif viral_ratio > 20:
            return 9.0   # Strong viral performance
        elif viral_ratio > 10:
            return 6.0   # Good viral performance
        elif viral_ratio > 5:
            return 3.0   # Some viral potential
        elif viral_ratio > 2:
            return 1.0   # Moderate viral potential
        else:
            return 0.0   # Normal performance

    def _calculate_subscriber_bonus(self, subscribers: int) -> float:
        """Calculate bonus points based on subscriber count"""
        if subscribers >= 1000000:
            return 10.0  # 8.1M channels get max bonus
        elif subscribers >= 500000:
            return 8.0
        elif subscribers >= 100000:
            return 6.0
        elif subscribers >= 50000:
            return 4.0
        elif subscribers >= 10000:
            return 2.0
        else:
            return 0.0

    def infer_demographics_enhanced(self, channel_data: Dict, videos_data: Dict, content_analysis: Dict) -> Dict:
        """Enhanced demographics inference"""
        content_categories = content_analysis.get('content_categories', {})
        channel_title = channel_data.get('channel_title', '').lower()
        
        # Adjust based on content type
        if any(cat in content_categories for cat in ['gaming', 'tech']):
            base_demographics = {
                'age_groups': {'13-17': 25, '18-24': 45, '25-34': 20, '35+': 10},
                'gender_ratio': {'male': 75, 'female': 25},
                'geographic_distribution': {'US': 35, 'UK': 15, 'India': 20, 'Other': 30},
            }
        elif any(cat in content_categories for cat in ['educational', 'tutorial']):
            base_demographics = {
                'age_groups': {'18-24': 35, '25-34': 40, '35-44': 15, '45+': 10},
                'gender_ratio': {'male': 55, 'female': 45},
                'geographic_distribution': {'US': 40, 'UK': 15, 'India': 15, 'Other': 30},
            }
        else:
            base_demographics = {
                'age_groups': {'18-24': 35, '25-34': 40, '35-44': 15, '45+': 10},
                'gender_ratio': {'male': 65, 'female': 35},
                'geographic_distribution': {'US': 40, 'UK': 15, 'India': 10, 'Other': 35},
            }
        
        # Infer interests based on content
        interests = []
        for category in content_categories:
            if category == 'gaming':
                interests.extend(['Video Games', 'Entertainment', 'Technology'])
            elif category == 'tech':
                interests.extend(['Technology', 'Gadgets', 'Innovation'])
            elif category == 'educational':
                interests.extend(['Education', 'Learning', 'Self-Improvement'])
            elif category == 'entertainment':
                interests.extend(['Entertainment', 'Comedy', 'Fun'])
        
        # Remove duplicates and limit
        interests = list(dict.fromkeys(interests))[:5]
        if not interests:
            interests = ['Technology', 'Education', 'Entertainment']
        
        base_demographics['interests'] = interests
        return base_demographics

    # REPLACE JUST THE AI INSIGHTS METHOD IN YOUR youtube_analyzer.py:

    def generate_ai_insights_enhanced(self, channel_data: Dict, ai_metrics: Dict, 
                                content_analysis: Dict, performance_metrics: Dict) -> List[Dict]:
        """Generate AI insights for ADMIN analysis with Groq API - FIXED"""
        try:
            # üî• Use Groq API for ADMIN insights with CORRECT data
            groq_insights = self.generate_groq_insights_admin(channel_data, ai_metrics, content_analysis, performance_metrics)
            if groq_insights and len(groq_insights) >= 3:
                logger.info(f"‚úÖ Groq generated {len(groq_insights)} ADMIN AI insights")
                return groq_insights
            else:
                logger.warning("‚ùå Groq returned insufficient insights for admin, using enhanced rule-based")
                
        except Exception as e:
            logger.error(f"‚ùå Groq API error in admin analysis: {str(e)}")
        
        # Fallback to enhanced rule-based insights for admin
        logger.info("üîÑ Using enhanced rule-based insights for admin")
        return self.generate_enhanced_rule_based_insights(channel_data, ai_metrics, content_analysis, performance_metrics)

    def generate_groq_insights_admin(self, channel_data: Dict, ai_metrics: Dict, 
                                content_analysis: Dict, performance_metrics: Dict) -> List[Dict]:
        """Generate insights using Groq API for ADMIN analysis - FIXED DATA SOURCES"""
        if not self.groq_keys:
            logger.warning("‚ùå No Groq API keys available for admin insights")
            return []
        
        # Try all Groq API keys
        for key_index, api_key in enumerate(self.groq_keys):
            try:
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
                
                # üî• FIXED: Use CORRECT data sources for admin analysis
                prompt_data = {
                    'channel_title': channel_data.get('channel_title', 'Unknown Channel'),
                    'subscribers': channel_data.get('subscribers', 0),
                    'total_views': channel_data.get('total_views', 0),
                    'total_videos': channel_data.get('total_videos', 0),
                    'channel_age_days': channel_data.get('channel_age_days', 0),
                    
                    # üî• FIX: Get engagement from the RIGHT place
                    'engagement_rate': performance_metrics.get('estimated_retention_rate', 0) / 10,  # Fallback calculation
                    
                    # üî• FIX: Top video data should come from engagement_metrics
                    'top_video_views': channel_data.get('top_performing_video_views', 0),
                    'top_video_title': channel_data.get('top_performing_video_title', ''),
                    
                    'performance_tier': ai_metrics.get('performance_tier', 'Unknown'),
                    'content_diversity': content_analysis.get('content_diversity_score', 0),
                    'optimal_length': performance_metrics.get('optimal_video_length', 'Unknown'),
                    'performance_trend': performance_metrics.get('performance_trend', 'Unknown'),
                    'growth_potential': ai_metrics.get('growth_potential', 'Unknown'),
                    'viral_ratio': ai_metrics.get('viral_ratio', 0)
                }
                
                # Calculate viral ratio if not available
                viral_ratio = prompt_data['viral_ratio'] or (prompt_data['top_video_views'] / max(prompt_data['subscribers'], 1))
                
                # üî• DEBUG: Log what we're sending to AI
                logger.info("=== AI PROMPT DATA DEBUG ===")
                logger.info(f"Engagement rate sent to AI: {prompt_data['engagement_rate']}%")
                logger.info(f"Subscribers: {prompt_data['subscribers']}")
                logger.info(f"Top video views: {prompt_data['top_video_views']}")
                logger.info(f"Viral ratio: {viral_ratio:.1f}X")
                
                prompt = f"""
                Analyze this YouTube channel from a BUSINESS OWNER perspective and provide exactly 3 STRATEGIC insights in JSON format:
                
                Channel Data (Business Owner's Channel):
                - Channel: {prompt_data['channel_title']}
                - Subscribers: {prompt_data['subscribers']}
                - Total Views: {prompt_data['total_views']}
                - Total Videos: {prompt_data['total_videos']}
                - Channel Age: {prompt_data['channel_age_days']} days
                - Engagement Rate: {prompt_data['engagement_rate']}% (industry avg: 4-6%)
                - Performance Tier: {prompt_data['performance_tier']}
                - Content Diversity: {prompt_data['content_diversity']}/100
                - Optimal Video Length: {prompt_data['optimal_length']}
                - Performance Trend: {prompt_data['performance_trend']}
                - Growth Potential: {prompt_data['growth_potential']}
                
                Top Video: "{prompt_data['top_video_title'][:100]}..."
                Top Video Views: {prompt_data['top_video_views']} (Viral Ratio: {viral_ratio:.1f}X subscribers)
                
                This is the BUSINESS OWNER'S channel. Provide STRATEGIC business-focused insights:
                
                Provide exactly 3 insights in this JSON format:
                {{
                    "insights": [
                        {{
                            "type": "business_growth|content_strategy|audience_development|monetization|brand_building",
                            "title": "Strategic business-focused title",
                            "description": "Detailed strategic explanation focusing on business growth and brand development",
                            "priority": "high|medium|low",
                            "confidence": 0.85,
                            "actionable_steps": ["strategic step 1", "business-focused step 2", "measurable step 3"]
                        }}
                    ]
                }}
                
                Focus on:
                - Business growth strategies
                - Brand development
                - Audience monetization
                - Content strategy for business
                - Long-term channel sustainability
                - Competitive advantage
                """

                payload = {
                    "messages": [{"role": "user", "content": prompt}],
                    "model": "llama-3.1-8b-instant",
                    "temperature": 0.7,
                    "max_tokens": 1500,
                    "stream": False
                }
                
                logger.info(f"ü§ñ Using Groq API key {key_index + 1}/{len(self.groq_keys)} for ADMIN insights")
                response = requests.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=45
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    logger.info(f"‚úÖ Groq API key {key_index + 1} ADMIN insights response received")
                    
                    try:
                        import re
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            json_str = json_match.group()
                            insights_data = json.loads(json_str)
                            insights = insights_data.get('insights', [])
                            
                            if len(insights) >= 3:
                                logger.info(f"‚úÖ Successfully parsed {len(insights)} ADMIN insights with key {key_index + 1}")
                                return insights
                            else:
                                logger.warning(f"‚ùå Groq key {key_index + 1} returned only {len(insights)} ADMIN insights")
                                continue  # Try next key
                        else:
                            logger.warning(f"‚ùå No JSON found in Groq ADMIN response with key {key_index + 1}")
                            continue  # Try next key
                            
                    except json.JSONDecodeError as e:
                        logger.error(f"‚ùå JSON parse error from Groq ADMIN key {key_index + 1}: {e}")
                        continue  # Try next key
                        
                elif response.status_code in [401, 403, 429]:
                    # API key issues: invalid, forbidden, rate limited
                    error_msg = response.json().get('error', {}).get('message', 'Unknown error')
                    logger.warning(f"‚ùå Groq API key {key_index + 1} failed for ADMIN: {error_msg}")
                    continue  # Try next key
                else:
                    logger.error(f"‚ùå Groq API key {key_index + 1} ADMIN call failed: {response.status_code}")
                    continue  # Try next key
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå Network error with Groq ADMIN key {key_index + 1}: {str(e)}")
                continue
            except Exception as e:
                logger.error(f"‚ùå Groq API ADMIN key {key_index + 1} error: {str(e)}")
                continue
        
        logger.error("‚ùå All Groq API keys exhausted for ADMIN insights")
        return []
        
    def generate_groq_insights_public(self, channel_data: Dict, ai_metrics: Dict, 
                                    content_analysis: Dict, performance_metrics: Dict,
                                    engagement_metrics: Dict) -> List[Dict]:
        """FIXED: Generate insights using Groq API with PROPER key rotation"""
        if not self.groq_keys:
            logger.warning("‚ùå No Groq API keys available")
            return []
        
        # Try all Groq API keys
        for key_index, api_key in enumerate(self.groq_keys):
            try:
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
                
                # Prepare prompt data (same as before)
                prompt_data = {
                    'channel_title': channel_data.get('channel_title', 'Unknown Channel'),
                    'subscribers': channel_data.get('subscribers', 0),
                    'total_views': channel_data.get('total_views', 0),
                    'total_videos': channel_data.get('total_videos', 0),
                    'channel_age_days': channel_data.get('channel_age_days', 0),
                    'top_video_views': engagement_metrics.get('top_performing_video_views', 0),
                    'top_video_title': engagement_metrics.get('top_performing_video_title', ''),
                    'engagement_rate': engagement_metrics.get('avg_engagement_rate', 0),
                    'performance_tier': ai_metrics.get('performance_tier', 'Unknown'),
                    'content_diversity': content_analysis.get('content_diversity_score', 0),
                    'optimal_length': performance_metrics.get('optimal_video_length', 'Unknown'),
                    'performance_trend': performance_metrics.get('performance_trend', 'Unknown'),
                    'growth_potential': ai_metrics.get('growth_potential', 'Unknown')
                }
                
                viral_ratio = prompt_data['top_video_views'] / max(prompt_data['subscribers'], 1)
                
                prompt = f"""
                Analyze this YouTube channel and provide exactly 3 actionable insights in JSON format:
                
                Channel Data:
                - Channel: {prompt_data['channel_title']}
                - Subscribers: {prompt_data['subscribers']}
                - Total Views: {prompt_data['total_views']}
                - Total Videos: {prompt_data['total_videos']}
                - Channel Age: {prompt_data['channel_age_days']} days
                - Engagement Rate: {prompt_data['engagement_rate']}% (industry avg: 4-6%)
                - Performance Tier: {prompt_data['performance_tier']}
                - Content Diversity: {prompt_data['content_diversity']}/100
                - Optimal Video Length: {prompt_data['optimal_length']}
                - Performance Trend: {prompt_data['performance_trend']}
                - Growth Potential: {prompt_data['growth_potential']}
                
                Top Video: "{prompt_data['top_video_title'][:100]}..."
                Top Video Views: {prompt_data['top_video_views']} (Viral Ratio: {viral_ratio:.1f}X subscribers)
                
                Provide exactly 3 insights in this JSON format:
                {{
                    "insights": [
                        {{
                            "type": "growth_opportunity|content_strategy|engagement_boost|audience_growth",
                            "title": "Short specific actionable title",
                            "description": "Detailed explanation focusing on actionable advice",
                            "priority": "high|medium|low",
                            "confidence": 0.85,
                            "actionable_steps": ["step1", "step2", "step3"]
                        }}
                    ]
                }}
                
                Focus on ACTUAL channel metrics and provide realistic, actionable insights.
                """
                
                payload = {
                    "messages": [{"role": "user", "content": prompt}],
                    "model": "llama-3.1-8b-instant",
                    "temperature": 0.7,
                    "max_tokens": 1500,
                    "stream": False
                }
                
                logger.info(f"ü§ñ Using Groq API key {key_index + 1}/{len(self.groq_keys)} for insights")
                response = requests.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=45
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    logger.info(f"‚úÖ Groq API key {key_index + 1} insights response received")
                    
                    try:
                        import re
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            json_str = json_match.group()
                            insights_data = json.loads(json_str)
                            insights = insights_data.get('insights', [])
                            
                            if len(insights) >= 3:
                                logger.info(f"‚úÖ Successfully parsed {len(insights)} insights with key {key_index + 1}")
                                return insights
                            
                    except json.JSONDecodeError as e:
                        logger.error(f"‚ùå JSON parse error from Groq key {key_index + 1}: {e}")
                        
                elif response.status_code in [401, 403, 429]:
                    # API key issues: invalid, forbidden, rate limited
                    error_msg = response.json().get('error', {}).get('message', 'Unknown error')
                    logger.warning(f"‚ùå Groq API key {key_index + 1} failed: {error_msg}")
                    continue  # Try next key
                else:
                    logger.error(f"‚ùå Groq API key {key_index + 1} call failed: {response.status_code}")
                    continue  # Try next key
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå Network error with Groq key {key_index + 1}: {str(e)}")
                continue
            except Exception as e:
                logger.error(f"‚ùå Groq API key {key_index + 1} error: {str(e)}")
                continue
        
        logger.error("‚ùå All Groq API keys exhausted for insights")
        return []

    def generate_enhanced_rule_based_insights(self, channel_data: Dict, ai_metrics: Dict,
                                        content_analysis: Dict, performance_metrics: Dict) -> List[Dict]:
        """IMPROVED insights focusing on your VIRAL POTENTIAL with correct data sources"""
        insights = []
        
        # üî• FIX: Use the CORRECT data sources
        subscribers = channel_data.get('subscribers', 0)
        top_video_views = channel_data.get('top_performing_video_views', 0)  # This should be from channel_data
        engagement_rate = ai_metrics.get('avg_engagement_rate', 0)
        engagement_consistency = channel_data.get('engagement_consistency', 0)
        
        # üî• KEY INSIGHT 1: VIRAL POTENTIAL
        viral_ratio = top_video_views / max(subscribers, 1)
        
        if viral_ratio > 50:  # Your ratio is 175x!
            insights.append({
                'type': 'growth_opportunity',
                'title': 'üöÄ EXPLOSIVE VIRAL POTENTIAL DETECTED',
                'description': f'Your top video has {top_video_views} views - that\'s {viral_ratio:.0f}X your subscriber count! This indicates massive content appeal to new audiences.',
                'priority': 'high',
                'confidence': 0.95,
                'actionable_steps': [
                    f'ANALYZE: Study why "{channel_data.get("top_performing_video_title", "your top video")}" went viral',
                    'REPLICATE: Create more content in the exact same style/format',
                    'PROMOTE: Feature this video in your channel trailer and pinned comment',
                    'CONVERT: Add strong subscriber calls-to-action in similar videos'
                ]
            })
        else:
            insights.append({
                'type': 'growth_opportunity',
                'title': 'Build Audience Foundation',
                'description': f'Focus on establishing your channel with {subscribers} subscribers and building your initial audience trust.',
                'priority': 'high',
                'confidence': 0.90,
                'actionable_steps': [
                    'Create consistent content schedule',
                    'Engage with every comment',
                    'Optimize video metadata',
                    'Share on relevant platforms'
                ]
            })
        
        # üî• KEY INSIGHT 2: ENGAGEMENT STRATEGY
        if engagement_rate < 3:
            insights.append({
                'type': 'engagement_boost',
                'title': 'Improve Audience Engagement',
                'description': f'Your engagement rate of {engagement_rate:.1f}% is below the YouTube average. Focus on increasing viewer interaction.',
                'priority': 'high',
                'confidence': 0.88,
                'actionable_steps': [
                    'Add clear calls-to-action in video intros and outros',
                    'Create interactive content like polls and Q&As',
                    'Respond to comments within 24 hours',
                    'Use YouTube Community tab for engagement'
                ]
            })
        else:
            insights.append({
                'type': 'engagement_boost',
                'title': 'Maintain Strong Engagement',
                'description': f'Your {engagement_rate:.1f}% engagement rate provides a solid foundation for growth.',
                'priority': 'medium',
                'confidence': 0.80,
                'actionable_steps': [
                    'Continue engaging with your loyal audience',
                    'Experiment with new interactive content formats',
                    'Analyze retention metrics weekly',
                    'Network with creators in your niche'
                ]
            })
        
        # üî• KEY INSIGHT 3: CONTENT STRATEGY
        optimal_length = performance_metrics.get('optimal_video_length', 'Unknown')
        insights.append({
            'type': 'content_strategy',
            'title': f'{optimal_length} Content Excellence',
            'description': f'Your optimal video length is {optimal_length.lower()} - perfect for audience retention and algorithm favorability.',
            'priority': 'high',
            'confidence': 0.88,
            'actionable_steps': [
                f'Maintain {optimal_length.lower()} format for new videos',
                'Focus on strong openings (first 15 seconds)',
                'Use chapter timestamps for better retention',
                'Create content series to encourage binge-watching'
            ]
        })
        
        logger.info(f"‚úÖ Generated {len(insights)} VIRAL-FOCUSED admin insights")
        return insights

    def predict_growth_enhanced(self, channel_data: Dict, ai_metrics: Dict, engagement_metrics: Dict) -> Dict:
        """Realistic growth prediction that actually works"""
        current_subs = channel_data.get('subscribers', 0)
        health_score = ai_metrics.get('channel_health_score', 0)
        engagement_rate = engagement_metrics.get('avg_engagement_rate', 0)
        consistency = engagement_metrics.get('performance_consistency', 0)
        content_quality = ai_metrics.get('content_quality_score', 0)
        viral_ratio = ai_metrics.get('viral_ratio', 0)
        growth_potential = ai_metrics.get('growth_potential', 'Unknown')
        
        # üî• REALISTIC base growth rates
        if current_subs >= 1000000:
            base_growth = 0.3  # Large channels grow slower
        elif current_subs >= 100000:
            base_growth = 0.8
        elif current_subs >= 10000:
            base_growth = 1.5
        elif current_subs >= 1000:
            base_growth = 2.5
        else:
            base_growth = 4.0  # Small channels can grow faster
        
        # Growth potential multiplier
        growth_multiplier = {
            'High': 1.8,
            'Medium-High': 1.4,
            'Medium': 1.0,
            'Low-Medium': 0.6,
            'Low': 0.3
        }.get(growth_potential, 1.0)
        
        # Viral content boost
        viral_multiplier = 1.0
        if viral_ratio > 10:
            viral_multiplier = 1.5  # 50% boost for viral channels
        elif viral_ratio > 5:
            viral_multiplier = 1.3  # 30% boost
        
        # Calculate monthly growth rate
        monthly_growth_rate = base_growth * growth_multiplier * viral_multiplier
        
        # Ensure minimum growth for active channels
        if engagement_rate > 3 and consistency > 50:
            monthly_growth_rate = max(monthly_growth_rate, 0.5)  # At least 0.5%
        
        # Cap growth rates for realism
        monthly_growth_rate = min(monthly_growth_rate, 15.0)  # Max 15% monthly growth
        
        # Calculate projected subscribers
        if monthly_growth_rate > 0:
            predicted_3month = int(current_subs * (1 + monthly_growth_rate/100) ** 3)
            predicted_6month = int(current_subs * (1 + monthly_growth_rate/100) ** 6)
            predicted_12month = int(current_subs * (1 + monthly_growth_rate/100) ** 12)
        else:
            # If no growth, maintain current subscribers
            predicted_3month = current_subs
            predicted_6month = current_subs
            predicted_12month = current_subs
        
        predictions = {
            'predicted_monthly_growth_rate': round(monthly_growth_rate, 1),
            'predicted_3month_subs': predicted_3month,
            'predicted_6month_subs': predicted_6month,
            'predicted_12month_subs': predicted_12month,
            'confidence': min(health_score / 100, 0.9),
            'growth_drivers': self.identify_growth_drivers(ai_metrics, engagement_metrics)
        }
        
        logger.info(f"üîÆ Growth prediction: {monthly_growth_rate}% monthly for {current_subs} subs")
        logger.info(f"Growth potential: {growth_potential}, Viral ratio: {viral_ratio:.1f}X")
        
        return predictions

    def identify_growth_drivers(self, ai_metrics: Dict, engagement_metrics: Dict) -> List[str]:
        """Identify key growth drivers"""
        drivers = []
        
        if ai_metrics.get('channel_health_score', 0) >= 70:
            drivers.append('Strong channel health')
        
        if engagement_metrics.get('avg_engagement_rate', 0) >= 6:
            drivers.append('High engagement rate')
        
        if engagement_metrics.get('performance_consistency', 0) >= 70:
            drivers.append('Consistent performance')
        
        if ai_metrics.get('content_quality_score', 0) >= 75:
            drivers.append('Quality content')
        
        if not drivers:
            drivers.append('Foundation building phase')
        
        return drivers[:3]
    

    def generate_recommendations_enhanced(self, channel_data: Dict, insights: List[Dict], 
                                       content_analysis: Dict, ai_metrics: Dict) -> List[Dict]:
        """Enhanced growth recommendations"""
        recommendations = []
        
        subscribers = channel_data.get('subscribers', 0)
        health_score = ai_metrics.get('channel_health_score', 0)
        content_diversity = content_analysis.get('content_diversity_score', 0)
        
        # Recommendation 1: Based on channel size
        if subscribers < 1000:
            recommendations.append({
                'type': 'foundation',
                'title': 'Build Audience Foundation',
                'description': 'Focus on establishing your channel identity and building initial audience trust.',
                'priority': 'high',
                'actionable_steps': [
                    'Define clear channel niche and value proposition',
                    'Create 5-10 pillar content pieces',
                    'Engage with every comment for first month',
                    'Optimize all video metadata (titles, descriptions, tags)'
                ]
            })
        
        # Recommendation 2: Based on content diversity
        if content_diversity < 60:
            recommendations.append({
                'type': 'content_strategy',
                'title': 'Diversify Content Approach',
                'description': 'Expand your content range to attract wider audience segments.',
                'priority': 'medium',
                'actionable_steps': [
                    'Add one new content category each month',
                    'Test different video formats (tutorials, reviews, vlogs)',
                    'Survey audience for content preferences',
                    'Analyze competitor content strategies'
                ]
            })
        
        # Recommendation 3: Based on channel health
        if health_score < 60:
            recommendations.append({
                'type': 'optimization',
                'title': 'Improve Core Metrics',
                'description': 'Focus on improving fundamental channel metrics before aggressive growth.',
                'priority': 'high',
                'actionable_steps': [
                    'Increase audience engagement through interactive content',
                    'Improve video retention with better editing and pacing',
                    'Establish consistent upload schedule',
                    'Analyze and address audience drop-off points'
                ]
            })
        
        # Ensure we have at least 3 recommendations
        while len(recommendations) < 3:
            recommendations.append({
                'type': 'general',
                'title': 'Continue Strategic Growth',
                'description': 'Maintain consistent content quality and audience engagement.',
                'priority': 'medium',
                'actionable_steps': [
                    'Monitor analytics regularly',
                    'Stay updated with platform algorithm changes',
                    'Engage with your community consistently',
                    'Experiment with new content ideas'
                ]
            })
        
        return recommendations[:3]
    
    def generate_recommendations_public(self, channel_data: Dict, insights: List[Dict], 
                                content_analysis: Dict, ai_metrics: Dict,
                                engagement_metrics: Dict) -> List[Dict]:
        """Generate recommendations using ACTUAL public channel data with proper rotation"""
        try:
            # üî• Try Groq API with ACTUAL channel data and proper rotation
            groq_recommendations = self.generate_groq_recommendations_public(channel_data, insights, content_analysis, ai_metrics, engagement_metrics)
            if groq_recommendations and len(groq_recommendations) >= 3:
                logger.info(f"‚úÖ Groq generated {len(groq_recommendations)} PUBLIC recommendations")
                return groq_recommendations
            else:
                logger.warning("‚ùå Groq returned insufficient recommendations, using public rule-based")
                
        except Exception as e:
            logger.error(f"‚ùå Groq API error for public recommendations: {str(e)}")
        
        # Fallback to public rule-based recommendations
        logger.info("üîÑ Using public rule-based recommendations")
        return self.generate_public_rule_based_recommendations(
            channel_data, insights, content_analysis, ai_metrics, engagement_metrics
        )
    
    def analyze_channel_public(self, channel_id: str, force_refresh: bool = True) -> Dict:
        """Clean public analysis with COMPREHENSIVE DEBUGGING"""
        try:
            logger.info(f"üéØ [DEBUG] Starting PUBLIC analysis for: {channel_id}")
            
            # STEP 1: Get channel data with detailed debugging
            logger.info(f"üîç [DEBUG] Step 1: Fetching channel data for: {channel_id}")
            channel_data = self.get_enhanced_channel_data_with_retry(channel_id)
            
            if not channel_data:
                logger.error(f"‚ùå [DEBUG] FAILED to get channel data for: {channel_id}")
                return self.get_public_fallback_analysis(channel_id)
            
            logger.info(f"‚úÖ [DEBUG] Channel data retrieved: {channel_data.get('channel_title')}")
            logger.info(f"üìä [DEBUG] Channel stats: {channel_data.get('subscribers')} subs, {channel_data.get('total_videos')} videos")
            
            # STEP 2: Get videos data with detailed debugging
            logger.info(f"üîç [DEBUG] Step 2: Fetching videos data for channel: {channel_data.get('channel_id')}")
            videos_data = self.get_channel_videos_safe(channel_data.get('channel_id'), max_results=30)
            
            logger.info(f"‚úÖ [DEBUG] Videos data retrieved: {videos_data.get('total_videos_fetched', 0)} videos")
            
            # DEBUG: Check what videos we actually got
            if videos_data and 'video_stats' in videos_data:
                items = videos_data['video_stats'].get('items', [])
                logger.info(f"üîç [DEBUG] Raw videos count: {len(items)}")
                
                if items:
                    for i, video in enumerate(items[:3]):  # Log first 3 videos
                        stats = video.get('statistics', {})
                        logger.info(f"üé¨ [DEBUG] Video {i+1}: {video.get('snippet', {}).get('title', 'Unknown')[:50]}...")
                        logger.info(f"    Views: {stats.get('viewCount', 0)}, Likes: {stats.get('likeCount', 0)}")
            
            # STEP 3: Calculate metrics with debugging
            logger.info(f"üîç [DEBUG] Step 3: Calculating engagement metrics")
            engagement_metrics = self.calculate_comprehensive_engagement(videos_data)
            
            logger.info(f"üìà [DEBUG] Engagement metrics calculated:")
            logger.info(f"    - Avg engagement rate: {engagement_metrics.get('avg_engagement_rate', 0)}%")
            logger.info(f"    - Top video views: {engagement_metrics.get('top_performing_video_views', 0)}")
            logger.info(f"    - Total likes: {engagement_metrics.get('total_recent_likes', 0)}")
            logger.info(f"    - Videos analyzed: {engagement_metrics.get('videos_analyzed', 0)}")
            
            # STEP 4: Continue with other calculations
            content_analysis = self.analyze_content_strategy_enhanced(videos_data, channel_data)
            performance_metrics = self.get_performance_metrics_safe(videos_data, engagement_metrics)
            
            # STEP 5: Generate AI insights
            ai_metrics = self.calculate_ai_enhanced_metrics(
                channel_data, engagement_metrics, content_analysis, performance_metrics
            )
            demographics = self.infer_demographics_enhanced(channel_data, videos_data, content_analysis)
            
            # STEP 6: Use ACTUAL data for insights and recommendations
            insights = self.generate_ai_insights_public(
                channel_data, ai_metrics, content_analysis, performance_metrics, engagement_metrics
            )
            predictions = self.predict_growth_enhanced(channel_data, ai_metrics, engagement_metrics)
            recommendations = self.generate_recommendations_public(channel_data, insights, content_analysis, ai_metrics, engagement_metrics)
            
            # Compile final analysis
            analysis = {
                'channel_metrics': channel_data,
                'engagement_metrics': engagement_metrics,
                'performance_metrics': performance_metrics,
                'content_analysis': content_analysis,
                'growth_metrics': self.calculate_growth_metrics(channel_data, engagement_metrics),
                'ai_enhanced_metrics': ai_metrics,
                'demographics': demographics,
                'ai_insights': insights,
                'growth_predictions': predictions,
                'recommendations': recommendations,
                'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
                'data_source': 'youtube_api_v3_public'
            }
            
            logger.info(f"üì¶ [DEBUG] Public analysis COMPLETED for channel {channel_id}")
            logger.info(f"‚úÖ [DEBUG] Final analysis - Subscribers: {analysis['channel_metrics'].get('subscribers', 0)}")
            logger.info(f"‚úÖ [DEBUG] Final analysis - Engagement: {analysis['engagement_metrics'].get('avg_engagement_rate', 0)}%")
            logger.info(f"‚úÖ [DEBUG] Final analysis - Top video: {analysis['engagement_metrics'].get('top_performing_video_views', 0)} views")
            
            return analysis
                    
        except Exception as e:
            logger.error(f"‚ùå [DEBUG] ERROR in public analysis: {str(e)}")
            import traceback
            logger.error(f"‚ùå [DEBUG] Traceback: {traceback.format_exc()}")
            return self.get_public_fallback_analysis(channel_id)
    
    def generate_public_rule_based_recommendations(self, channel_data: Dict, insights: List[Dict],
                                            content_analysis: Dict, ai_metrics: Dict,
                                            engagement_metrics: Dict) -> List[Dict]:
        """Public rule-based recommendations using ACTUAL channel data"""
        recommendations = []
        
        subscribers = channel_data.get('subscribers', 0)
        top_video_views = engagement_metrics.get('top_performing_video_views', 0)
        viral_ratio = top_video_views / max(subscribers, 1)
        content_diversity = content_analysis.get('content_diversity_score', 0)
        publishing_frequency = content_analysis.get('publishing_frequency', 'Unknown')
        engagement_rate = engagement_metrics.get('avg_engagement_rate', 0)
        
        # üî• STRATEGIC RECOMMENDATION 1: CONTENT STRATEGY
        if viral_ratio > 50:
            recommendations.append({
                'type': 'content_strategy',
                'title': 'Leverage Viral Success Pattern',
                'description': f'Your viral video ({viral_ratio:.0f}X subscriber ratio) reveals a winning content formula. Systematically replicate this success.',
                'priority': 'high',
                'actionable_steps': [
                    'Deconstruct the viral video: analyze title, thumbnail, content structure, and audience reactions',
                    'Create a content series based on the viral video format',
                    'Develop 3-5 variations of the successful concept',
                    'A/B test thumbnails and titles for new similar content'
                ],
                'expected_impact': 'Consistent high-performing content and accelerated subscriber growth',
                'timeline': 'short-term'
            })
        else:
            recommendations.append({
                'type': 'content_strategy',
                'title': 'Develop Signature Content Series',
                'description': 'Create recurring content series to build audience anticipation and loyalty.',
                'priority': 'high',
                'actionable_steps': [
                    'Identify 2-3 core content formats that align with your niche',
                    'Create weekly/bi-weekly series with consistent branding',
                    'Develop content calendar with theme-based episodes',
                    'Use series-specific thumbnails and intro sequences'
                ],
                'expected_impact': 'Improved audience retention and predictable viewership patterns',
                'timeline': 'medium-term'
            })
        
        # üî• STRATEGIC RECOMMENDATION 2: AUDIENCE GROWTH STRATEGY
        if engagement_rate < 4:
            recommendations.append({
                'type': 'audience_growth',
                'title': 'Boost Engagement & Community Building',
                'description': 'Focus on increasing viewer interaction to build a loyal community and improve algorithm favorability.',
                'priority': 'high',
                'actionable_steps': [
                    'Add clear calls-to-action in every video (likes, comments, subscriptions)',
                    'Respond to comments within 24 hours to build community',
                    'Create interactive content like polls, Q&As, and challenges',
                    'Use YouTube Community tab for between-video engagement'
                ],
                'expected_impact': '20-30% increase in engagement rate and subscriber loyalty',
                'timeline': 'short-term'
            })
        else:
            recommendations.append({
                'type': 'audience_growth',
                'title': 'Expand Audience Reach',
                'description': 'Leverage current engagement to reach new audience segments and platforms.',
                'priority': 'medium',
                'actionable_steps': [
                    'Create short-form content clips for TikTok/Instagram Reels',
                    'Collaborate with similar-sized channels for cross-promotion',
                    'Optimize video SEO with targeted keywords and descriptions',
                    'Run targeted YouTube ads with high-performing content'
                ],
                'expected_impact': 'Increased subscriber growth rate and broader audience reach',
                'timeline': 'medium-term'
            })
        
        # üî• STRATEGIC RECOMMENDATION 3: CHANNEL OPTIMIZATION
        if content_diversity < 60:
            recommendations.append({
                'type': 'optimization',
                'title': 'Strategic Content Portfolio Expansion',
                'description': 'Diversify content while maintaining channel focus to capture broader audience segments.',
                'priority': 'medium',
                'actionable_steps': [
                    'Add one new content category every 6-8 weeks',
                    'Conduct audience survey to identify desired content types',
                    'Create "pillar-cluster" content model around main topics',
                    'Test seasonal or trending content variations'
                ],
                'expected_impact': 'Broader audience appeal and reduced content fatigue',
                'timeline': 'long-term'
            })
        else:
            recommendations.append({
                'type': 'optimization',
                'title': 'Advanced YouTube SEO & Discovery Optimization',
                'description': 'Maximize channel discoverability through advanced optimization techniques.',
                'priority': 'medium',
                'actionable_steps': [
                    'Implement comprehensive keyword strategy across all metadata',
                    'Optimize video chapters for better search visibility',
                    'Create and promote channel trailers for new visitors',
                    'Develop content clusters around high-performing topics'
                ],
                'expected_impact': 'Increased organic search traffic and recommended views',
                'timeline': 'medium-term'
            })
        
        logger.info(f"‚úÖ Generated {len(recommendations)} public rule-based recommendations")
        return recommendations

    def generate_basic_recommendations_fallback(self) -> List[Dict]:
        """Basic fallback recommendations for public analysis"""
        return [
            {
                'type': 'foundation',
                'title': 'Build Strong Content Foundation',
                'description': 'Focus on establishing consistent content quality and audience engagement.',
                'priority': 'high',
                'actionable_steps': [
                    'Create consistent upload schedule',
                    'Engage with audience comments regularly',
                    'Optimize video titles and descriptions',
                    'Analyze basic YouTube analytics weekly'
                ],
                'expected_impact': 'Stable audience growth and improved channel authority',
                'timeline': 'short-term'
            },
            {
                'type': 'growth',
                'title': 'Develop Audience Growth Strategy',
                'description': 'Implement systematic approaches to attract and retain subscribers.',
                'priority': 'medium',
                'actionable_steps': [
                    'Research and implement effective SEO strategies',
                    'Create shareable content for social media',
                    'Network with similar content creators',
                    'Run viewer engagement campaigns'
                ],
                'expected_impact': 'Increased subscriber acquisition and retention',
                'timeline': 'medium-term'
            },
            {
                'type': 'optimization',
                'title': 'Optimize Channel Performance',
                'description': 'Continuously improve channel metrics through data analysis.',
                'priority': 'medium',
                'actionable_steps': [
                    'Monitor audience retention metrics',
                    'Test different thumbnail and title combinations',
                    'Analyze competitor performance benchmarks',
                    'Stay updated with YouTube algorithm changes'
                ],
                'expected_impact': 'Improved engagement rates and algorithm favorability',
                'timeline': 'ongoing'
            }
        ]

    def generate_public_rule_based_insights(self, channel_data: Dict, ai_metrics: Dict,
                                        content_analysis: Dict, performance_metrics: Dict,
                                        engagement_metrics: Dict) -> List[Dict]:
        """Public rule-based insights using ACTUAL channel data"""
        insights = []
        
        subscribers = channel_data.get('subscribers', 0)
        top_video_views = engagement_metrics.get('top_performing_video_views', 0)
        engagement_rate = engagement_metrics.get('avg_engagement_rate', 0)
        performance_trend = performance_metrics.get('performance_trend', 'Unknown')
        content_diversity = content_analysis.get('content_diversity_score', 0)
        
        # üî• INSIGHT 1: VIRAL POTENTIAL
        viral_ratio = top_video_views / max(subscribers, 1)
        
        if viral_ratio > 50:
            insights.append({
                'type': 'growth_opportunity',
                'title': 'üöÄ Explosive Viral Potential',
                'description': f'Your top video has {top_video_views} views - {viral_ratio:.0f}X your subscriber count! This indicates massive content appeal to new audiences.',
                'priority': 'high',
                'confidence': 0.95,
                'actionable_steps': [
                    f'Analyze why "{engagement_metrics.get("top_performing_video_title", "your top video")}" resonated so well',
                    'Create more content in the same successful format',
                    'Promote this video in your channel trailer and community posts',
                    'Add strong subscriber calls-to-action in similar videos'
                ]
            })
        else:
            insights.append({
                'type': 'growth_opportunity',
                'title': 'Build Audience Foundation',
                'description': f'Focus on establishing your channel with {subscribers} subscribers and building initial audience trust.',
                'priority': 'high',
                'confidence': 0.90,
                'actionable_steps': [
                    'Create consistent content schedule',
                    'Engage with every comment to build community',
                    'Optimize video metadata for better discovery',
                    'Share content on relevant platforms'
                ]
            })
        
        # üî• INSIGHT 2: ENGAGEMENT STRATEGY
        if engagement_rate < 4:
            insights.append({
                'type': 'engagement_boost',
                'title': 'Improve Audience Engagement',
                'description': f'Your engagement rate of {engagement_rate}% is below the YouTube average. Focus on increasing viewer interaction.',
                'priority': 'high',
                'confidence': 0.85,
                'actionable_steps': [
                    'Add clear calls-to-action in video intros and outros',
                    'Create interactive content like polls and Q&As',
                    'Respond to comments promptly to build community',
                    'Use YouTube Community tab for engagement between uploads'
                ]
            })
        else:
            insights.append({
                'type': 'engagement_boost',
                'title': 'Maintain Strong Engagement',
                'description': f'Your {engagement_rate}% engagement rate provides a solid foundation for growth.',
                'priority': 'medium',
                'confidence': 0.80,
                'actionable_steps': [
                    'Continue engaging with your loyal audience',
                    'Experiment with new interactive content formats',
                    'Analyze retention metrics to identify drop-off points',
                    'Network with creators in your niche'
                ]
            })
        
        # üî• INSIGHT 3: CONTENT STRATEGY
        if content_diversity < 60:
            insights.append({
                'type': 'content_strategy',
                'title': 'Diversify Content Approach',
                'description': 'Expanding your content range can help attract wider audience segments and reduce viewer fatigue.',
                'priority': 'medium',
                'confidence': 0.78,
                'actionable_steps': [
                    'Add one new content category each month',
                    'Survey your audience for content preferences',
                    'Test different video formats and lengths',
                    'Analyze competitor content strategies'
                ]
            })
        else:
            insights.append({
                'type': 'content_strategy',
                'title': 'Optimize Content Performance',
                'description': 'Your diverse content portfolio provides opportunities for strategic optimization.',
                'priority': 'medium',
                'confidence': 0.75,
                'actionable_steps': [
                    'Double down on your best-performing content categories',
                    'Create content series around successful topics',
                    'Optimize publishing schedule based on audience activity',
                    'Monitor trending topics in your niche'
                ]
            })
        
        logger.info(f"‚úÖ Generated {len(insights)} public rule-based insights")
        return insights
    

    def get_public_fallback_analysis(self, channel_id: str) -> Dict:
        """Public fallback analysis with no admin data"""
        logger.warning(f"üîÑ Using public fallback analysis for channel {channel_id}")
        
        return {
            'channel_metrics': {
                'channel_title': 'YouTube Channel',
                'subscribers': 0,
                'total_views': 0,
                'total_videos': 0,
                'channel_age_days': 0,
                'country': 'Unknown',
                'custom_url': '',
            },
            'engagement_metrics': {
                'avg_engagement_rate': 0,
                'engagement_health': 'Unknown',
                'total_recent_likes': 0,
                'total_recent_comments': 0,
                'videos_analyzed': 0,
                'avg_views_per_video': 0,
                'avg_likes_per_video': 0,
                'avg_comments_per_video': 0,
                'performance_consistency': 0,
                'views_std_dev': 0,
                'top_performing_video_views': 0,
                'top_performing_video_likes': 0,
                'top_performing_video_title': '',
                'engagement_consistency': 0,
            },
            'performance_metrics': {
                'avg_duration_seconds': 0,
                'optimal_video_length': 'Unknown',
                'performance_trend': 'Unknown',
                'avg_performance_score': 0,
                'duration_consistency': 0,
                'estimated_retention_rate': 0,
                'content_velocity_score': 0,
            },
            'ai_enhanced_metrics': {
                'channel_health_score': 0,
                'performance_tier': 'Unknown',
                'growth_potential': 'Unknown',
                'content_quality_score': 0,
            },
            'content_analysis': {
                'content_categories': {},
                'publishing_frequency': 'Unknown',
                'optimal_video_length': 'Unknown',
                'content_gaps': [],
                'trending_alignment_score': 0,
                'content_diversity_score': 0,
                'total_videos_analyzed': 0,
            },
            'demographics': {
                'age_groups': {'18-24': 35, '25-34': 40, '35-44': 15, '45+': 10},
                'gender_ratio': {'male': 65, 'female': 35},
                'geographic_distribution': {'US': 40, 'UK': 15, 'India': 10, 'Other': 35},
                'interests': ['Technology', 'Education', 'Entertainment'],
            },
            'ai_insights': self.generate_basic_insights_fallback(),
            'growth_predictions': {},
            'recommendations': self.generate_basic_recommendations_fallback(),  # üî• FIXED METHOD NAME
            'data_source': 'public_fallback'
        }
    def generate_ai_insights_public(self, channel_data: Dict, ai_metrics: Dict, 
                              content_analysis: Dict, performance_metrics: Dict,
                              engagement_metrics: Dict) -> List[Dict]:
        """Generate AI insights using ACTUAL channel data - NO admin interference"""
        try:
            # üî• Use Groq API with ACTUAL channel data
            groq_insights = self.generate_groq_insights_public(channel_data, ai_metrics, content_analysis, performance_metrics, engagement_metrics)
            if groq_insights and len(groq_insights) >= 3:
                logger.info(f"‚úÖ Groq generated {len(groq_insights)} PUBLIC AI insights")
                return groq_insights
            else:
                logger.warning("‚ùå Groq returned insufficient insights, using public rule-based")
                
        except Exception as e:
            logger.error(f"‚ùå Groq API error in public analysis: {str(e)}")
        
        # Fallback to public rule-based insights
        logger.info("üîÑ Using public rule-based insights")
        return self.generate_public_rule_based_insights(
            channel_data, ai_metrics, content_analysis, performance_metrics, engagement_metrics
        )

    def generate_groq_insights_public(self, channel_data: Dict, ai_metrics: Dict, 
                                    content_analysis: Dict, performance_metrics: Dict,
                                    engagement_metrics: Dict) -> List[Dict]:
        """FIXED: Generate insights using Groq API with PROPER key rotation"""
        if not self.groq_keys:
            logger.warning("‚ùå No Groq API keys available")
            return self.generate_public_rule_based_insights(channel_data, ai_metrics, content_analysis, performance_metrics, engagement_metrics)
        
        # Try all Groq API keys
        for key_index, api_key in enumerate(self.groq_keys):
            try:
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
                
                # Prepare prompt data
                prompt_data = {
                    'channel_title': channel_data.get('channel_title', 'Unknown Channel'),
                    'subscribers': channel_data.get('subscribers', 0),
                    'total_views': channel_data.get('total_views', 0),
                    'total_videos': channel_data.get('total_videos', 0),
                    'channel_age_days': channel_data.get('channel_age_days', 0),
                    'top_video_views': engagement_metrics.get('top_performing_video_views', 0),
                    'top_video_title': engagement_metrics.get('top_performing_video_title', ''),
                    'engagement_rate': engagement_metrics.get('avg_engagement_rate', 0),
                    'performance_tier': ai_metrics.get('performance_tier', 'Unknown'),
                    'content_diversity': content_analysis.get('content_diversity_score', 0),
                    'optimal_length': performance_metrics.get('optimal_video_length', 'Unknown'),
                    'performance_trend': performance_metrics.get('performance_trend', 'Unknown'),
                    'growth_potential': ai_metrics.get('growth_potential', 'Unknown')
                }
                
                viral_ratio = prompt_data['top_video_views'] / max(prompt_data['subscribers'], 1)
                
                prompt = f"""
                Analyze this YouTube channel and provide exactly 3 actionable insights in JSON format:
                
                Channel Data:
                - Channel: {prompt_data['channel_title']}
                - Subscribers: {prompt_data['subscribers']}
                - Total Views: {prompt_data['total_views']}
                - Total Videos: {prompt_data['total_videos']}
                - Channel Age: {prompt_data['channel_age_days']} days
                - Engagement Rate: {prompt_data['engagement_rate']}% (industry avg: 4-6%)
                - Performance Tier: {prompt_data['performance_tier']}
                - Content Diversity: {prompt_data['content_diversity']}/100
                - Optimal Video Length: {prompt_data['optimal_length']}
                - Performance Trend: {prompt_data['performance_trend']}
                - Growth Potential: {prompt_data['growth_potential']}
                
                Top Video: "{prompt_data['top_video_title'][:100]}..."
                Top Video Views: {prompt_data['top_video_views']} (Viral Ratio: {viral_ratio:.1f}X subscribers)
                
                Provide exactly 3 insights in this JSON format:
                {{
                    "insights": [
                        {{
                            "type": "growth_opportunity|content_strategy|engagement_boost|audience_growth",
                            "title": "Short specific actionable title",
                            "description": "Detailed explanation focusing on actionable advice",
                            "priority": "high|medium|low",
                            "confidence": 0.85,
                            "actionable_steps": ["step1", "step2", "step3"]
                        }}
                    ]
                }}
                
                Focus on ACTUAL channel metrics and provide realistic, actionable insights.
                """
                
                payload = {
                    "messages": [{"role": "user", "content": prompt}],
                    "model": "llama-3.1-8b-instant",
                    "temperature": 0.7,
                    "max_tokens": 1500,
                    "stream": False
                }
                
                logger.info(f"ü§ñ Using Groq API key {key_index + 1}/{len(self.groq_keys)} for insights")
                response = requests.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=45
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    logger.info(f"‚úÖ Groq API key {key_index + 1} insights response received")
                    
                    try:
                        import re
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            json_str = json_match.group()
                            insights_data = json.loads(json_str)
                            insights = insights_data.get('insights', [])
                            
                            if len(insights) >= 3:
                                logger.info(f"‚úÖ Successfully parsed {len(insights)} insights with key {key_index + 1}")
                                return insights
                            else:
                                logger.warning(f"‚ùå Groq key {key_index + 1} returned only {len(insights)} insights")
                                continue  # Try next key
                        else:
                            logger.warning(f"‚ùå No JSON found in Groq response with key {key_index + 1}")
                            continue  # Try next key
                            
                    except json.JSONDecodeError as e:
                        logger.error(f"‚ùå JSON parse error from Groq key {key_index + 1}: {e}")
                        continue  # Try next key
                        
                elif response.status_code in [401, 403, 429]:
                    # API key issues: invalid, forbidden, rate limited
                    error_msg = response.json().get('error', {}).get('message', 'Unknown error')
                    logger.warning(f"‚ùå Groq API key {key_index + 1} failed: {error_msg}")
                    continue  # Try next key
                else:
                    logger.error(f"‚ùå Groq API key {key_index + 1} call failed: {response.status_code}")
                    continue  # Try next key
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå Network error with Groq key {key_index + 1}: {str(e)}")
                continue
            except Exception as e:
                logger.error(f"‚ùå Groq API key {key_index + 1} error: {str(e)}")
                continue
        
        logger.error("‚ùå All Groq API keys exhausted for insights")
        # Fallback to rule-based insights
        logger.info("üîÑ Using public rule-based insights")
        return self.generate_public_rule_based_insights(channel_data, ai_metrics, content_analysis, performance_metrics, engagement_metrics)

    def generate_groq_recommendations_public(self, channel_data: Dict, insights: List[Dict], 
                                        content_analysis: Dict, ai_metrics: Dict,
                                        engagement_metrics: Dict) -> List[Dict]:
        """FIXED: Generate strategic recommendations using Groq API with PROPER key rotation"""
        if not self.groq_keys:
            logger.warning("‚ùå No Groq API keys available")
            return self.generate_public_rule_based_recommendations(channel_data, insights, content_analysis, ai_metrics, engagement_metrics)
        
        # Try all Groq API keys
        for key_index, api_key in enumerate(self.groq_keys):
            try:
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
                
                # Use ACTUAL public channel data
                prompt_data = {
                    'channel_title': channel_data.get('channel_title', 'Unknown Channel'),
                    'subscribers': channel_data.get('subscribers', 0),
                    'total_views': channel_data.get('total_views', 0),
                    'total_videos': channel_data.get('total_videos', 0),
                    'channel_age_days': channel_data.get('channel_age_days', 0),
                    'engagement_rate': engagement_metrics.get('avg_engagement_rate', 0),
                    'performance_tier': ai_metrics.get('performance_tier', 'Unknown'),
                    'growth_potential': ai_metrics.get('growth_potential', 'Unknown'),
                    'content_diversity': content_analysis.get('content_diversity_score', 0),
                    'publishing_frequency': content_analysis.get('publishing_frequency', 'Unknown'),
                    'top_video_views': engagement_metrics.get('top_performing_video_views', 0),
                    'viral_ratio': engagement_metrics.get('top_performing_video_views', 0) / max(channel_data.get('subscribers', 1), 1)
                }
                
                prompt = f"""
                Based on this ACTUAL YouTube channel analysis, provide exactly 3 STRATEGIC RECOMMENDATIONS in JSON format.

                Channel Context:
                - Channel: {prompt_data['channel_title']}
                - Subscribers: {prompt_data['subscribers']}
                - Total Views: {prompt_data['total_views']}
                - Channel Age: {prompt_data['channel_age_days']} days
                - Engagement Rate: {prompt_data['engagement_rate']}%
                - Performance Tier: {prompt_data['performance_tier']}
                - Growth Potential: {prompt_data['growth_potential']}
                - Content Diversity: {prompt_data['content_diversity']}/100
                - Publishing Frequency: {prompt_data['publishing_frequency']}
                - Top Video Views: {prompt_data['top_video_views']} (Viral Ratio: {prompt_data['viral_ratio']:.1f}X)

                Provide exactly 3 STRATEGIC RECOMMENDATIONS in this JSON format:
                {{
                    "recommendations": [
                        {{
                            "type": "content_strategy|audience_growth|monetization|optimization|collaboration",
                            "title": "Specific strategic recommendation title",
                            "description": "Detailed strategic explanation with clear implementation path",
                            "priority": "high|medium|low",
                            "actionable_steps": ["concrete step 1", "measurable step 2", "trackable step 3"],
                            "expected_impact": "What success looks like",
                            "timeline": "short-term|medium-term|long-term"
                        }}
                    ]
                }}

                Focus on STRATEGIC areas:
                1. Content Strategy & Planning
                2. Audience Growth & Retention  
                3. Channel Optimization & Monetization
                4. Collaboration & Networking
                5. Analytics & Performance Tracking

                Make recommendations:
                - SPECIFIC and actionable
                - MEASURABLE outcomes
                - REALISTIC for current channel size
                - FOCUSED on growth levers
                - Include clear implementation steps
                """

                payload = {
                    "messages": [{"role": "user", "content": prompt}],
                    "model": "llama-3.1-8b-instant",
                    "temperature": 0.7,
                    "max_tokens": 2000,
                    "stream": False
                }
                
                logger.info(f"ü§ñ Using Groq API key {key_index + 1}/{len(self.groq_keys)} for recommendations")
                response = requests.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=45
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    logger.info(f"‚úÖ Groq API key {key_index + 1} recommendations response received")
                    
                    try:
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            json_str = json_match.group()
                            recommendations_data = json.loads(json_str)
                            recommendations = recommendations_data.get('recommendations', [])
                            
                            if len(recommendations) >= 3:
                                logger.info(f"‚úÖ Successfully parsed {len(recommendations)} recommendations with key {key_index + 1}")
                                return recommendations
                            else:
                                logger.warning(f"‚ùå Groq key {key_index + 1} returned only {len(recommendations)} recommendations")
                                continue  # Try next key
                        else:
                            logger.warning(f"‚ùå No JSON found in Groq response with key {key_index + 1}")
                            continue  # Try next key
                            
                    except json.JSONDecodeError as e:
                        logger.error(f"‚ùå JSON parse error from Groq key {key_index + 1}: {e}")
                        continue  # Try next key
                        
                elif response.status_code in [401, 403, 429]:
                    # API key issues: invalid, forbidden, rate limited
                    error_msg = response.json().get('error', {}).get('message', 'Unknown error')
                    logger.warning(f"‚ùå Groq API key {key_index + 1} failed: {error_msg}")
                    continue  # Try next key
                else:
                    logger.error(f"‚ùå Groq API key {key_index + 1} call failed: {response.status_code}")
                    continue  # Try next key
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå Network error with Groq key {key_index + 1}: {str(e)}")
                continue
            except Exception as e:
                logger.error(f"‚ùå Groq API key {key_index + 1} error: {str(e)}")
                continue
        
        logger.error("‚ùå All Groq API keys exhausted for recommendations")
        # Fallback to rule-based recommendations
        logger.info("üîÑ Using public rule-based recommendations")
        return self.generate_public_rule_based_recommendations(channel_data, insights, content_analysis, ai_metrics, engagement_metrics)

        
    def calculate_growth_metrics(self, channel_data: Dict, engagement_metrics: Dict) -> Dict:
        """Calculate additional growth metrics"""
        subscribers = channel_data.get('subscribers', 0)
        total_views = channel_data.get('total_views', 0)
        channel_age = channel_data.get('channel_age_days', 1)
        
        return {
            'views_per_subscriber': round(total_views / max(subscribers, 1), 1),
            'subscribers_per_day': round(subscribers / max(channel_age, 1), 2),
            'views_per_day': round(total_views / max(channel_age, 1), 1),
            'engagement_velocity': engagement_metrics.get('avg_engagement_rate', 0) * 
                                 engagement_metrics.get('performance_consistency', 0) / 100
        }

    def generate_basic_insights_fallback(self) -> List[Dict]:
        """Generate basic insights for fallback scenarios - NOW 3 INSIGHTS"""
        return [
            {
                'type': 'growth_opportunity',
                'title': 'Establish Channel Foundation',
                'description': 'Focus on building a strong content foundation and engaging with your initial audience.',
                'priority': 'high',
                'confidence': 0.85,
                'actionable_steps': [
                    'Create consistent content schedule',
                    'Engage with audience comments',
                    'Optimize video titles and descriptions',
                    'Analyze basic YouTube analytics'
                ]
            },
            {
                'type': 'content_strategy', 
                'title': 'Develop Content Strategy',
                'description': 'Build a clear content strategy that aligns with your channel goals and audience interests.',
                'priority': 'medium',
                'confidence': 0.80,
                'actionable_steps': [
                    'Define your target audience',
                    'Research content topics in your niche',
                    'Create content calendar',
                    'Test different video formats'
                ]
            },
            {
                'type': 'engagement_boost',
                'title': 'Boost Audience Engagement',
                'description': 'Increase viewer interaction and build a loyal community around your content.',
                'priority': 'medium',
                'confidence': 0.78,
                'actionable_steps': [
                    'Use calls-to-action in videos',
                    'Respond to comments promptly',
                    'Create interactive content (polls, Q&A)',
                    'Run viewer challenges or contests'
                ]
            }
        ]

    # ==================== CONTENT ANALYSIS HELPERS ====================
    
    def categorize_content_ai_enhanced(self, videos: List[Dict]) -> Dict:
        """AI-enhanced content categorization"""
        categories = Counter()
        
        for video in videos:
            title = video['snippet']['title'].lower()
            description = video['snippet']['description'].lower()
            tags = video['snippet'].get('tags', [])
            
            # Advanced AI-powered categorization
            category_scores = self.analyze_content_with_ai_patterns(title, description, tags)
            for category, score in category_scores.items():
                categories[category] += score
        
        return dict(categories.most_common(10))

    def analyze_content_with_ai_patterns(self, title: str, description: str, tags: List[str]) -> Dict:
        """Analyze content using AI-inspired pattern matching"""
        scores = {}
        
        # Comprehensive category patterns with weighted scoring
        patterns = {
            'tutorial': {
                'patterns': ['tutorial', 'how to', 'guide', 'learn', 'step by step', 'walkthrough', 'explain', 'teach'],
                'weight': 1.2,
            },
            'review': {
                'patterns': ['review', 'unboxing', 'test', 'compared', 'vs', 'comparison', 'opinion', 'thoughts'],
                'weight': 1.1,
            },
            'educational': {
                'patterns': ['explained', 'education', 'learning', 'facts', 'science', 'knowledge', 'study', 'research'],
                'weight': 1.0,
            },
            'entertainment': {
                'patterns': ['funny', 'comedy', 'prank', 'challenge', 'meme', 'hilarious', 'entertainment', 'fun'],
                'weight': 0.9,
            },
            'tech': {
                'patterns': ['technology', 'tech', 'computer', 'software', 'app', 'digital', 'ai', 'programming'],
                'weight': 1.1,
            },
            'gaming': {
                'patterns': ['gameplay', 'walkthrough', 'gaming', 'playthrough', 'episode', 'game', 'stream'],
                'weight': 0.9,
            },
            'vlog': {
                'patterns': ['vlog', 'day in life', 'behind the scenes', 'my life', 'daily', 'update'],
                'weight': 0.8,
            },
            'news': {
                'patterns': ['news', 'update', 'announcement', 'breaking', 'latest', 'report', 'coverage'],
                'weight': 1.0,
            }
        }
        
        for category, config in patterns.items():
            score = 0
            # Check title matches
            for pattern in config['patterns']:
                if pattern in title:
                    score += config['weight'] * 2
                    break
            
            # Check description matches
            for pattern in config['patterns']:
                if pattern in description:
                    score += config['weight'] * 1.5
                    break
            
            # Check tag matches
            for tag in tags:
                tag_lower = tag.lower()
                for pattern in config['patterns']:
                    if pattern in tag_lower:
                        score += config['weight']
                        break
            
            if score > 0:
                scores[category] = score
        
        return scores

    def analyze_publishing_pattern_enhanced(self, videos: List[Dict]) -> Dict:
        """Enhanced publishing pattern analysis"""
        if len(videos) < 2:
            return {'frequency': 'Irregular', 'consistency': 0}
        
        try:
            dates = []
            for video in videos:
                publish_time = video['snippet']['publishedAt']
                publish_date = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
                dates.append(publish_date)
            
            dates.sort()
            
            if len(dates) >= 2:
                total_days = (dates[-1] - dates[0]).days
                if total_days > 0:
                    avg_days_between = total_days / (len(dates) - 1)
                    
                    # Calculate consistency
                    intervals = []
                    for i in range(1, len(dates)):
                        interval = (dates[i] - dates[i-1]).days
                        intervals.append(interval)
                    
                    consistency = self.calculate_consistency(intervals)
                    
                    if avg_days_between <= 1:
                        frequency = 'Daily'
                    elif avg_days_between <= 3:
                        frequency = 'Frequent (2-3 days)'
                    elif avg_days_between <= 7:
                        frequency = 'Weekly'
                    elif avg_days_between <= 14:
                        frequency = 'Bi-weekly'
                    elif avg_days_between <= 30:
                        frequency = 'Monthly'
                    else:
                        frequency = 'Irregular'
                    
                    return {'frequency': frequency, 'consistency': consistency}
            
            return {'frequency': 'Irregular', 'consistency': 0}
        except Exception:
            return {'frequency': 'Unknown', 'consistency': 0}

    def analyze_titles_ai_enhanced(self, videos: List[Dict]) -> Dict:
        """Enhanced title analysis with AI-inspired patterns"""
        titles = [video['snippet']['title'] for video in videos]
        
        avg_length = np.mean([len(title) for title in titles])
        has_emojis = any(any(char in title for char in ['üî•', 'üíØ', 'üéØ', '‚ö°', '‚ú®']) for title in titles)
        
        # Analyze common patterns
        common_words = Counter()
        for title in titles:
            words = re.findall(r'\b\w+\b', title.lower())
            common_words.update(words)
        
        common_patterns = [word for word, count in common_words.most_common(10) if count > 1 and len(word) > 3]
        
        # Calculate optimization score
        optimization_score = min(
            (70 if 40 <= avg_length <= 60 else 50) +
            (15 if has_emojis else 0) +
            (15 if len(common_patterns) >= 3 else 0),
            100
        )
        
        return {
            'avg_title_length': round(avg_length, 1),
            'has_emojis': has_emojis,
            'common_patterns': common_patterns[:5],
            'optimization_score': optimization_score
        }

    def identify_content_gaps_ai(self, categories: Dict, channel_data: Dict) -> List[str]:
        """AI-inspired content gap identification"""
        all_categories = {
            'educational': ['tutorial', 'explainer', 'how-to', 'guide', 'lesson'],
            'entertainment': ['comedy', 'challenge', 'prank', 'funny', 'entertainment'],
            'review': ['review', 'unboxing', 'test', 'comparison', 'opinion'],
            'news': ['news', 'update', 'announcement', 'breaking', 'report'],
            'vlog': ['vlog', 'day in life', 'behind scenes', 'personal', 'update'],
            'gaming': ['gameplay', 'walkthrough', 'stream', 'gaming', 'playthrough'],
            'tech': ['technology', 'tech', 'software', 'hardware', 'digital'],
            'creative': ['art', 'design', 'creative', 'making', 'build']
        }
        
        current_categories = set(categories.keys())
        gaps = []
        
        for main_category, sub_categories in all_categories.items():
            if main_category not in current_categories:
                gap_score = sum(1 for sub in sub_categories if self.is_relevant_gap(sub, channel_data))
                if gap_score >= 2:
                    gaps.append(main_category)
        
        return gaps[:4]

    def is_relevant_gap(self, sub_category: str, channel_data: Dict) -> bool:
        """Determine if a content gap is relevant to the channel"""
        channel_desc = (channel_data.get('channel_description', '') + 
                       channel_data.get('channel_title', '')).lower()
        
        relevance_keywords = {
            'tutorial': ['learn', 'teach', 'education', 'skill'],
            'review': ['product', 'service', 'compare', 'quality'],
            'tech': ['technology', 'software', 'digital', 'computer'],
            'gaming': ['game', 'play', 'entertainment', 'fun']
        }
        
        for keyword, related in relevance_keywords.items():
            if sub_category in keyword:
                return any(word in channel_desc for word in related)
        
        return True

    def assess_trending_alignment_ai(self, videos: List[Dict]) -> float:
        """AI-inspired trending alignment assessment"""
        if not videos:
            return 0
        
        recent_videos = videos[:10]
        current_date = datetime.now(timezone.utc)
        
        trending_score = 0
        max_score = len(recent_videos) * 10
        
        for video in recent_videos:
            publish_time = video['snippet']['publishedAt']
            publish_date = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
            
            days_ago = (current_date - publish_date).days
            recency_score = max(0, 10 - days_ago)
            
            stats = video.get('statistics', {})
            views = int(stats.get('viewCount', 0))
            performance_score = min(views / 1000, 5)
            
            trending_score += recency_score + performance_score
        
        return round((trending_score / max_score) * 100, 1) if max_score > 0 else 0
    
    def assess_consistency_fixed(self, consistency_score: float) -> str:
        """FIXED: Realistic consistency assessment"""
        if consistency_score >= 80:
            return 'Excellent'
        elif consistency_score >= 60:
            return 'Good'
        elif consistency_score >= 40:
            return 'Average'
        elif consistency_score >= 20:
            return 'Below Average'
        else:
            return 'Poor'  # 25.8% should be "Poor"

    def calculate_diversity_score_enhanced(self, categories: Dict) -> float:
        """Enhanced diversity score calculation"""
        if not categories:
            return 0
        
        num_categories = len(categories)
        total_videos = sum(categories.values())
        
        if total_videos == 0:
            return 0
        
        proportions = [count/total_videos for count in categories.values()]
        entropy = -sum(p * np.log(p) for p in proportions if p > 0)
        max_entropy = np.log(num_categories)
        
        base_diversity = (entropy / max_entropy) * 100 if max_entropy > 0 else 0
        
        strong_categories = sum(1 for p in proportions if p >= 0.15)
        balance_bonus = min(strong_categories * 10, 20)
        
        return round(min(base_diversity + balance_bonus, 100), 1)
    
    def predict_growth_realistic(self, channel_data: Dict, engagement_metrics: Dict) -> Dict:
        """FIXED: Realistic growth prediction for inactive channels"""
        subscribers = channel_data.get('subscribers', 0)
        content_velocity = engagement_metrics.get('content_velocity_score', 0)
        
        # If channel is inactive (content_velocity < 20), growth should be minimal
        if content_velocity < 20:
            monthly_growth = max(0.1, subscribers * 0.001)  # 0.1% or minimal growth
        else:
            # Use your existing growth calculation for active channels
            monthly_growth = self.predict_growth_enhanced(channel_data, engagement_metrics)
        
        return monthly_growth
    
    def assess_content_velocity_fixed(self, velocity_score: float) -> str:
        """FIXED: Realistic content velocity assessment"""
        if velocity_score >= 80:
            return 'Very Active'
        elif velocity_score >= 60:
            return 'Active'
        elif velocity_score >= 40:
            return 'Moderate'
        elif velocity_score >= 20:
            return 'Low'
        else:
            return 'Inactive'  # 3/100 should be "Inactive"

    def assess_content_freshness(self, videos: List[Dict]) -> float:
        """Assess content freshness based on publishing recency"""
        if not videos:
            return 0
        
        current_date = datetime.now(timezone.utc)
        freshness_scores = []
        
        for video in videos[:20]:
            publish_time = video['snippet']['publishedAt']
            publish_date = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
            
            days_ago = (current_date - publish_date).days
            freshness = max(0, 100 - (days_ago * 5))
            freshness_scores.append(freshness)
        
        return round(np.mean(freshness_scores), 1) if freshness_scores else 0